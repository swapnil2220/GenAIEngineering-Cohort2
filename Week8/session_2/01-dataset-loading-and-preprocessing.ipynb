{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 1: Dataset Loading and Preprocessing Pipelines\n",
    "========================================\n",
    "\n",
    "[Click to view on Google Colab](https://colab.research.google.com/drive/1cS4MUQx4Zl_5b9Z3UFQCE3maEUnLpjFn?usp=sharing)\n",
    "\n",
    "This notebook demonstrates practical approaches to loading, analyzing, and preprocessing real multimodal datasets for AI applications. We'll work with the [MSR-VTT](https://huggingface.co/datasets/friedrichor/MSR-VTT) video captioning dataset to explore the complete pipeline from raw data to training-ready batches.\n",
    "\n",
    "Learning Objectives:\n",
    "- Master real-world multimodal dataset loading and handling techniques\n",
    "- Understand dataset structure analysis and preprocessing pipeline design\n",
    "- Learn text tokenization and augmentation strategies for multimodal systems\n",
    "- Implement efficient custom dataset classes and data loaders\n",
    "- Optimize preprocessing pipelines for performance and memory efficiency using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy torch transformers datasets opencv-python\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Fix tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Real-World Dataset Loading: MSR-VTT Video Captioning Dataset\n",
    "\n",
    "**From Raw Data to Structured Datasets: Practical Multimodal Data Loading**\n",
    "\n",
    "Loading real multimodal datasets presents unique challenges that differ significantly from toy datasets. The MSR-VTT (Microsoft Research Video to Text) dataset serves as an excellent example of large-scale multimodal data with complex structure and real-world characteristics.\n",
    "\n",
    "**Key Concepts Explored:**\n",
    "1. **Large-Scale Dataset Management**\n",
    "   - MSR-VTT contains 10,000 video clips with 200,000 natural language descriptions\n",
    "   - Multiple dataset splits (train_7k, train_9k, test_1k) for different experimental setups\n",
    "   - Memory-efficient loading techniques for datasets \n",
    "\n",
    "2. **Dataset Structure Understanding**\n",
    "   - HuggingFace datasets integration for standardized data access\n",
    "   - DatasetDict navigation and split selection strategies\n",
    "   - Feature schema exploration to understand available data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msr_vtt_dataset(split: str = \"train_7k\", sample_size: Optional[int] = 100):\n",
    "    \"\"\"\n",
    "    Load MSR-VTT dataset with optional sampling for educational purposes.\n",
    "    \n",
    "    MSR-VTT (Microsoft Research Video to Text) is a large-scale video captioning dataset\n",
    "    containing 10,000 video clips with 200,000 natural language descriptions.\n",
    "    \n",
    "    Args:\n",
    "        split: Dataset config to load ('train_7k', 'train_9k', 'test_1k')\n",
    "        sample_size: Number of samples to load (None for full dataset)\n",
    "    \n",
    "    Returns:\n",
    "        Dataset object with video paths and captions\n",
    "    \"\"\"\n",
    "    print(f\"Loading MSR-VTT dataset - split: {split}\")\n",
    "    \n",
    "    # Load the dataset with config as split parameter\n",
    "    dataset_dict = load_dataset(\n",
    "        \"friedrichor/MSR-VTT\", \n",
    "        split,  # This is the config name (train_7k, train_9k, test_1k)\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Available splits: {list(dataset_dict.keys())}\")\n",
    "    \n",
    "    # Extract the actual dataset from the DatasetDict\n",
    "    if 'train' in dataset_dict:\n",
    "        dataset = dataset_dict['train']\n",
    "    else:\n",
    "        # If no 'train' split, use the first available split\n",
    "        first_split = list(dataset_dict.keys())[0]\n",
    "        dataset = dataset_dict[first_split]\n",
    "        print(f\"Using split '{first_split}' (no 'train' split found)\")\n",
    "    \n",
    "    print(f\"Original dataset size: {len(dataset)}\")\n",
    "    \n",
    "    # Sample subset for educational purposes\n",
    "    if sample_size and sample_size < len(dataset):\n",
    "        indices = random.sample(range(len(dataset)), sample_size)\n",
    "        dataset = dataset.select(indices)\n",
    "        print(f\"Sampled dataset size: {len(dataset)}\")\n",
    "    \n",
    "    # Display dataset structure\n",
    "    print(\"\\nDataset structure:\")\n",
    "    print(f\"Features: {dataset.features}\")\n",
    "    \n",
    "    # Show example\n",
    "    example = dataset[0]\n",
    "    print(f\"\\nExample entry:\")\n",
    "    print(f\"Video ID: {example.get('video_id', 'N/A')}\")\n",
    "    print(f\"Caption: {example.get('caption', 'N/A')}\")\n",
    "    print(f\"Available keys: {list(example.keys())}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Test the function\n",
    "dataset = load_msr_vtt_dataset(split=\"train_7k\", sample_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset Structure Analysis and Characterization\n",
    "\n",
    "**Deep Dive into Multimodal Data Characteristics**\n",
    "\n",
    "Understanding the structure and characteristics of multimodal datasets is crucial for designing effective preprocessing pipelines. This analysis phase reveals patterns, distributions, and potential challenges that will inform all subsequent processing decisions.\n",
    "\n",
    "**Key Concepts Explored:**\n",
    "1. **Statistical Data Profiling**\n",
    "   - Caption length distribution analysis revealing natural language patterns\n",
    "   - Video-caption relationship mapping to understand data redundancy\n",
    "\n",
    "2. **Text Content Analysis**\n",
    "   - Word count statistics providing insights into caption complexity\n",
    "   - Vocabulary diversity assessment for tokenization strategy planning\n",
    "   - Special character and multilingual content detection\n",
    "\n",
    "3. **Data Relationship Mapping**\n",
    "   - Video-to-caption ratio analysis revealing dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure(dataset):\n",
    "    \"\"\"\n",
    "    Analyze the structure and characteristics of the loaded dataset.\n",
    "    \n",
    "    Understanding dataset structure is crucial for preprocessing pipeline design:\n",
    "    - Data types and formats in each field\n",
    "    - Distribution of text lengths and video properties\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset object\n",
    "    \"\"\"\n",
    "    print(\"=== Dataset Structure Analysis ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    print(f\"Features: {list(dataset.features.keys())}\")\n",
    "    \n",
    "    # First, let's examine the structure of captions\n",
    "    sample_caption = dataset[0]['caption']\n",
    "    print(f\"\\nCaption field type: {type(sample_caption)}\")\n",
    "    print(f\"Sample caption: {sample_caption}\")\n",
    "    \n",
    "    # Handle different caption formats\n",
    "    if isinstance(sample_caption, list):\n",
    "        # If captions are lists, flatten them or take first element\n",
    "        captions = []\n",
    "        for item in dataset:\n",
    "            caption_list = item['caption']\n",
    "            if isinstance(caption_list, list) and len(caption_list) > 0:\n",
    "                captions.append(caption_list[0])  # Take first caption\n",
    "            else:\n",
    "                captions.append(str(caption_list))\n",
    "    else:\n",
    "        # If captions are strings\n",
    "        captions = [item['caption'] for item in dataset]\n",
    "    \n",
    "    # Calculate caption lengths\n",
    "    caption_lengths = [len(str(caption).split()) for caption in captions]\n",
    "    \n",
    "    print(f\"\\n=== Caption Analysis ===\")\n",
    "    print(f\"Average caption length: {np.mean(caption_lengths):.2f} words\")\n",
    "    print(f\"Min caption length: {min(caption_lengths)} words\")\n",
    "    print(f\"Max caption length: {max(caption_lengths)} words\")\n",
    "    print(f\"Median caption length: {np.median(caption_lengths):.2f} words\")\n",
    "    \n",
    "    # Analyze video IDs\n",
    "    video_ids = [item['video_id'] for item in dataset]\n",
    "    unique_videos = len(set(video_ids))\n",
    "    print(f\"\\n=== Video Analysis ===\")\n",
    "    print(f\"Total samples: {len(video_ids)}\")\n",
    "    print(f\"Unique videos: {unique_videos}\")\n",
    "    print(f\"Average captions per video: {len(video_ids) / unique_videos:.2f}\")\n",
    "    \n",
    "    # Check for categories if available\n",
    "    if 'category' in dataset.features:\n",
    "        categories = [item['category'] for item in dataset]\n",
    "        unique_categories = set(categories)\n",
    "        print(f\"\\n=== Category Analysis ===\")\n",
    "        print(f\"Unique categories: {len(unique_categories)}\")\n",
    "        print(f\"Categories: {sorted(unique_categories)}\")\n",
    "    \n",
    "    # Check for other fields\n",
    "    sample = dataset[0]\n",
    "    print(f\"\\n=== Available Fields ===\")\n",
    "    for key, value in sample.items():\n",
    "        print(f\"{key}: {type(value)} - {str(value)[:100]}...\")\n",
    "    \n",
    "    return {\n",
    "        'caption_lengths': caption_lengths,\n",
    "        'unique_videos': unique_videos,\n",
    "        'sample_fields': list(sample.keys()),\n",
    "        'total_samples': len(dataset),\n",
    "        'processed_captions': captions\n",
    "    }\n",
    "\n",
    "# Analyze the dataset\n",
    "analysis = analyze_dataset_structure(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Advanced Text Preprocessing for Multimodal Systems\n",
    "\n",
    "**From Raw Text to Model-Ready Tokens: Multimodal Text Processing**\n",
    "\n",
    "Text preprocessing in multimodal systems requires sophisticated approaches that consider how text will interact with other modalities. Unlike single-modal text processing, multimodal preprocessing must ensure consistency, efficiency, and compatibility across different data types.\n",
    "\n",
    "**Key Concepts Explored:**\n",
    "1. **Tokenization Strategy Design**\n",
    "   - BERT-style tokenization for transformer-based multimodal models\n",
    "   - Subword tokenization handling out-of-vocabulary terms effectively\n",
    "   - Special token integration ([CLS], [SEP]) for multimodal alignment\n",
    "   - Vocabulary consistency across different text sources and domains\n",
    "\n",
    "2. **Sequence Length Optimization**\n",
    "   - Maximum length determination balancing information retention and efficiency\n",
    "   - Padding strategies ensuring consistent tensor dimensions for batch processing\n",
    "   - Truncation policies preserving the most important textual information\n",
    "   - Dynamic length handling for variable-content scenarios\n",
    "\n",
    "3. **Attention Mechanism Preparation**\n",
    "   - Attention mask generation distinguishing real content from padding\n",
    "   - Token importance weighting for multimodal attention mechanisms\n",
    "   - Position encoding compatibility with visual and audio modalities\n",
    "   - Cross-modal attention preparation through consistent tokenization\n",
    "\n",
    "**Learning Outcomes:**\n",
    "Learners will understand how text preprocessing in multimodal systems differs from single-modal approaches, master the implementation of efficient tokenization pipelines, and learn to optimize text processing for integration with visual and audio data. This knowledge is essential for building robust multimodal AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_captions(captions: List[str], tokenizer_name: str = \"bert-base-uncased\", \n",
    "                           max_length: int = 77) -> Dict:\n",
    "    \"\"\"\n",
    "    Preprocess text captions for multimodal learning.\n",
    "    \n",
    "    Text preprocessing in multimodal systems involves:\n",
    "    - Tokenization: Converting text to tokens that models can understand\n",
    "    - Length normalization: Ensuring consistent sequence lengths\n",
    "    - Special tokens: Adding [CLS], [SEP] tokens for BERT-style models\n",
    "    - Attention masks: Indicating which tokens are padding vs. real content\n",
    "    \n",
    "    Args:\n",
    "        captions: List of text captions\n",
    "        tokenizer_name: HuggingFace tokenizer to use\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with tokenized captions and attention masks\n",
    "    \"\"\"\n",
    "    print(f\"Preprocessing {len(captions)} captions with {tokenizer_name}\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    # Show original captions\n",
    "    print(\"\\nOriginal captions (first 3):\")\n",
    "    for i, caption in enumerate(captions[:3]):\n",
    "        print(f\"{i+1}: {caption}\")\n",
    "    \n",
    "    # Tokenize captions\n",
    "    tokenized = tokenizer(\n",
    "        captions,\n",
    "        padding=True,           # Pad to same length\n",
    "        truncation=True,        # Truncate if too long\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"     # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTokenized output shape:\")\n",
    "    print(f\"Input IDs: {tokenized['input_ids'].shape}\")\n",
    "    print(f\"Attention mask: {tokenized['attention_mask'].shape}\")\n",
    "    \n",
    "    # Show tokenized example\n",
    "    print(f\"\\nTokenized example (first caption):\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized['input_ids'][0])\n",
    "    print(f\"Tokens: {tokens[:15]}...\")  # Show first 15 tokens\n",
    "    \n",
    "    # Analyze tokenization statistics\n",
    "    token_lengths = tokenized['attention_mask'].sum(dim=1)\n",
    "    print(f\"\\nTokenization statistics:\")\n",
    "    print(f\"Average tokens per caption: {token_lengths.float().mean():.2f}\")\n",
    "    print(f\"Max tokens used: {token_lengths.max().item()}\")\n",
    "    print(f\"Min tokens used: {token_lengths.min().item()}\")\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask'],\n",
    "        'tokenizer': tokenizer,\n",
    "        'token_lengths': token_lengths\n",
    "    }\n",
    "\n",
    "# Test text preprocessing with actual captions from dataset\n",
    "sample_captions = analysis['processed_captions'][:10]  # Use processed captions from analysis\n",
    "text_data = preprocess_text_captions(sample_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Custom Multimodal Dataset Implementation\n",
    "\n",
    "**Building Efficient and Scalable Dataset Classes for Multimodal Learning**\n",
    "\n",
    "Custom dataset classes form the backbone of efficient multimodal training pipelines. Unlike simple data containers, these classes must handle complex multimodal data relationships and provide consistent interfaces for diverse data types.\n",
    "\n",
    "**Key Concepts Explored:**\n",
    "1. **Multimodal Data Architecture**\n",
    "   - Consistent indexing ensuring perfect alignment between modalities\n",
    "   - Flexible preprocessing integration allowing different transforms per modality\n",
    "   - Error handling strategies gracefully managing missing or corrupted data\n",
    "\n",
    "2. **Memory Management Strategies**\n",
    "   - On-demand data loading minimizing memory footprint during training\n",
    "   - Efficient tensor creation and management for GPU compatibility\n",
    "\n",
    "4. **Performance Optimization Techniques**\n",
    "   - Batch optimization strategies minimizing data transfer overhead\n",
    "\n",
    "5. **Extensibility and Modularity**\n",
    "   - Modular design patterns enabling easy extension to new modalities\n",
    "   - Configuration-driven preprocessing allowing runtime customization\n",
    "   - Plugin architecture supporting custom transformation pipelines\n",
    "   - Version compatibility ensuring dataset classes work across different frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_videos(dataset, num_videos=1, output_dir=\"videos\"):\n",
    "    \"\"\"Download videos from dataset URLs.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    downloaded = []\n",
    "    \n",
    "    for i in range(min(num_videos, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        video_id = sample['video_id']\n",
    "        url = sample['url']\n",
    "        start_time = sample.get('start time', 0)\n",
    "        end_time = sample.get('end time', 30)\n",
    "        \n",
    "        output_path = os.path.join(output_dir, f\"{video_id}.mp4\")\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            downloaded.append(output_path)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            cmd = [\n",
    "                \"yt-dlp\", \"--format\", \"best[height<=480]\", \n",
    "                \"--output\", output_path, \"--quiet\",\n",
    "                \"--external-downloader\", \"ffmpeg\",\n",
    "                \"--external-downloader-args\", \n",
    "                f\"ffmpeg:-ss {start_time} -t {end_time - start_time}\",\n",
    "                url\n",
    "            ]\n",
    "            \n",
    "            result = subprocess.run(cmd, capture_output=True, timeout=120)\n",
    "            if result.returncode == 0 and os.path.exists(output_path):\n",
    "                downloaded.append(output_path)\n",
    "                print(f\"Downloaded: {video_id}\")\n",
    "        except:\n",
    "            print(f\"Failed: {video_id}\")\n",
    "    \n",
    "    return downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"Minimal multimodal dataset with all HF dataset keys using cv2.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, video_dir=\"videos\", max_frames=8):\n",
    "        self.dataset = dataset\n",
    "        self.video_dir = video_dir\n",
    "        self.max_frames = max_frames\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Find valid samples\n",
    "        self.valid_indices = []\n",
    "        for i in range(len(dataset)):\n",
    "            video_id = dataset[i]['video_id']\n",
    "            if os.path.exists(os.path.join(video_dir, f\"{video_id}.mp4\")):\n",
    "                self.valid_indices.append(i)\n",
    "        \n",
    "        print(f\"Found {len(self.valid_indices)} valid samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[self.valid_indices[idx]]\n",
    "        \n",
    "        # Process text\n",
    "        caption = sample.get('caption', '')\n",
    "        if isinstance(caption, list):\n",
    "            caption = caption[0]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            caption, padding='max_length', truncation=True, \n",
    "            max_length=77, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Load video frames using cv2\n",
    "        video_path = os.path.join(self.video_dir, f\"{sample['video_id']}.mp4\")\n",
    "        video_frames = self._load_video_cv2(video_path)\n",
    "        \n",
    "        return {\n",
    "            'text_ids': tokenized['input_ids'].squeeze(0),\n",
    "            'text_mask': tokenized['attention_mask'].squeeze(0),\n",
    "            'caption': caption,\n",
    "            'video_frames': video_frames,\n",
    "            'video_id': sample['video_id'],\n",
    "            'video': sample['video'],\n",
    "            'source': sample['source'],\n",
    "            'category': sample['category'],\n",
    "            'url': sample['url'],\n",
    "            'start_time': sample['start time'],\n",
    "            'end_time': sample['end time'],\n",
    "            'id': sample['id']\n",
    "        }\n",
    "    \n",
    "    def _load_video_cv2(self, video_path):\n",
    "        \"\"\"Load video frames using OpenCV with comprehensive preprocessing.\n",
    "        \n",
    "        This function loads a video file, extracts frames uniformly across the video duration,\n",
    "        preprocesses them for machine learning (resizing, color conversion, normalization),\n",
    "        and returns them as a PyTorch tensor suitable for multimodal model training.\n",
    "        \n",
    "        Args:\n",
    "            video_path (str): Full path to the video file (.mp4, .avi, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (max_frames, 3, 224, 224) containing\n",
    "                        RGB video frames normalized to [0, 1] range.\n",
    "                        \n",
    "        Raises:\n",
    "            Exception: If video cannot be loaded, falls back to placeholder frames.\n",
    "            \n",
    "        Processing Pipeline:\n",
    "            1. Open video file using OpenCV VideoCapture\n",
    "            2. Extract video metadata (total frames, fps)\n",
    "            3. Calculate uniform frame sampling indices\n",
    "            4. Extract and preprocess each frame:\n",
    "            - Convert from BGR to RGB color space\n",
    "            - Resize to 224x224 pixels (standard vision model input size)\n",
    "            - Normalize pixel values from [0, 255] to [0, 1] range\n",
    "            - Convert to PyTorch tensor with channels-first format (C, H, W)\n",
    "            5. Handle frame padding to ensure consistent output size\n",
    "            6. Stack all frames into a single tensor\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Initialize OpenCV VideoCapture object\n",
    "            # VideoCapture is the primary interface for reading video files in OpenCV\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            \n",
    "            # Step 2: Verify video file can be opened\n",
    "            # isOpened() returns True if the video source has been initialized successfully\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Could not open video: {video_path}\")\n",
    "                return None\n",
    "            \n",
    "            # Step 3: Extract video metadata for frame sampling strategy\n",
    "            # CAP_PROP_FRAME_COUNT: Total number of frames in the video\n",
    "            # CAP_PROP_FPS: Frames per second of the video\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            \n",
    "            # Step 4: Validate video properties\n",
    "            # Videos with 0 frames or invalid fps cannot be processed\n",
    "            if total_frames <= 0 or fps <= 0:\n",
    "                print(f\"Invalid video properties: frames={total_frames}, fps={fps}\")\n",
    "                cap.release()  # Always release resources\n",
    "                return None\n",
    "            \n",
    "            # Step 5: Calculate frame sampling indices for uniform temporal coverage\n",
    "            # We want to sample max_frames uniformly across the entire video duration\n",
    "            # This ensures we capture the video's temporal progression regardless of length\n",
    "            if total_frames <= self.max_frames:\n",
    "                # If video has fewer frames than needed, use all available frames\n",
    "                frame_indices = list(range(total_frames))\n",
    "            else:\n",
    "                # Calculate uniformly spaced indices across the video\n",
    "                # Formula: index = (sample_position * total_frames) / max_frames\n",
    "                # This gives us evenly distributed frames across the entire video\n",
    "                frame_indices = [\n",
    "                    int(i * total_frames / self.max_frames) \n",
    "                    for i in range(self.max_frames)\n",
    "                ]\n",
    "            \n",
    "            frames = []\n",
    "            \n",
    "            # Step 6: Extract and preprocess each sampled frame\n",
    "            for frame_idx in frame_indices:\n",
    "                # Step 6a: Seek to specific frame position\n",
    "                # CAP_PROP_POS_FRAMES sets the 0-based index of the frame to be decoded/captured next\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                \n",
    "                # Step 6b: Read the frame at current position\n",
    "                # ret: boolean indicating if frame was read successfully\n",
    "                # frame: numpy array containing the frame data (H, W, C) in BGR format\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                # Step 6c: Verify frame was read successfully\n",
    "                if ret and frame is not None:\n",
    "                    # Step 6d: Color space conversion from BGR to RGB\n",
    "                    # OpenCV uses BGR (Blue-Green-Red) by default, but most ML models expect RGB\n",
    "                    # This is crucial for correct color representation in the model\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Step 6e: Resize frame to standard input size\n",
    "                    # 224x224 is the standard input size for many vision models (ResNet, ViT, etc.)\n",
    "                    # Bicubic interpolation preserves image quality during resizing\n",
    "                    frame = cv2.resize(frame, (224, 224))\n",
    "                    \n",
    "                    # Step 6f: Convert to PyTorch tensor and normalize\n",
    "                    # Convert numpy array to PyTorch tensor with float32 precision\n",
    "                    # Permute dimensions from (H, W, C) to (C, H, W) - channels first format\n",
    "                    # Normalize pixel values from [0, 255] to [0, 1] range for numerical stability\n",
    "                    frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "                    \n",
    "                    frames.append(frame_tensor)\n",
    "                else:\n",
    "                    # Step 6g: Handle frame reading failure\n",
    "                    print(f\"Could not read frame {frame_idx}\")\n",
    "                    break  # Stop processing if we encounter read errors\n",
    "            \n",
    "            # Step 7: Release video capture resources\n",
    "            # Always release to prevent memory leaks and file locks\n",
    "            cap.release()\n",
    "            \n",
    "            # Step 8: Handle frame padding to ensure consistent output size\n",
    "            # If we have fewer frames than max_frames, pad with the last valid frame\n",
    "            # This ensures all samples have the same tensor dimensions for batching\n",
    "            while len(frames) < self.max_frames:\n",
    "                if frames:\n",
    "                    # Clone the last frame to avoid tensor sharing issues\n",
    "                    frames.append(frames[-1].clone())\n",
    "                else:\n",
    "                    # If no frames were successfully read, return None\n",
    "                    return None\n",
    "            \n",
    "            # Step 9: Ensure exact frame count by truncating if necessary\n",
    "            # This handles edge cases where we might have extracted more frames than needed\n",
    "            frames = frames[:self.max_frames]\n",
    "            \n",
    "            # Step 10: Stack individual frame tensors into a single tensor\n",
    "            # Result shape: (max_frames, 3, 224, 224)\n",
    "            # This creates a 4D tensor suitable for video processing models\n",
    "            return torch.stack(frames)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Step 11: Comprehensive error handling\n",
    "            # Catch any unexpected errors (codec issues, corrupted files, etc.)\n",
    "            print(f\"Error loading video {video_path}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size=1):\n",
    "    \"\"\"Create dataloader with all dataset keys.\"\"\"\n",
    "    def collate_fn(batch):\n",
    "        return {\n",
    "            'text_ids': torch.stack([item['text_ids'] for item in batch]),\n",
    "            'text_masks': torch.stack([item['text_mask'] for item in batch]),\n",
    "            'captions': [item['caption'] for item in batch],\n",
    "            'video_frames': torch.stack([item['video_frames'] for item in batch]),\n",
    "            'video_ids': [item['video_id'] for item in batch],\n",
    "            'videos': [item['video'] for item in batch],\n",
    "            'sources': [item['source'] for item in batch],\n",
    "            'categories': [item['category'] for item in batch],\n",
    "            'urls': [item['url'] for item in batch],\n",
    "            'start_times': [item['start_time'] for item in batch],\n",
    "            'end_times': [item['end_time'] for item in batch],\n",
    "            'ids': [item['id'] for item in batch]\n",
    "        }\n",
    "    \n",
    "    return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: \n",
    "# A number of videos present in the dataset are not available on youtube\n",
    "# anymore, hence the downloads will fail. \n",
    "# In such cases rerun the entire notebook so that a different section of the dataset \n",
    "# gets loaded or increase the value of `num_videos` so that chances of finding an \n",
    "# available video increases.\n",
    "\n",
    "# Usage\n",
    "downloaded_paths = download_videos(dataset, num_videos=2, output_dir=\"videos\")\n",
    "\n",
    "if downloaded_paths:\n",
    "    multimodal_dataset = MultimodalDataset(dataset, video_dir=\"videos\")\n",
    "    dataloader = create_dataloader(multimodal_dataset, batch_size=2)\n",
    "    \n",
    "    # Test\n",
    "    batch = next(iter(dataloader))\n",
    "    print(\"Available keys:\")\n",
    "    for key in batch.keys():\n",
    "        print(f\"  - {key}\")\n",
    "    \n",
    "    print(f\"\\nBatch size: {len(batch['captions'])}\")\n",
    "    print(f\"Video shape: {batch['video_frames'].shape}\")\n",
    "    print(f\"Text shape: {batch['text_ids'].shape}\")\n",
    "    print(f\"Video frames range: [{batch['video_frames'].min():.3f}, {batch['video_frames'].max():.3f}]\")\n",
    "    \n",
    "    # Display data for ALL videos in the batch\n",
    "    for i in range(len(batch['captions'])):\n",
    "        print(f\"\\n=== Video {i+1} ===\")\n",
    "        print(f\"URL: {batch['urls'][i]}\")\n",
    "        print(f\"Caption: {batch['captions'][i]}\")\n",
    "        print(f\"Video ID: {batch['video_ids'][i]}\")\n",
    "        print(f\"Source: {batch['sources'][i]}\")\n",
    "        print(f\"Category: {batch['categories'][i]}\")\n",
    "        print(f\"Start time: {batch['start_times'][i]}\")\n",
    "        print(f\"End time: {batch['end_times'][i]}\")\n",
    "        print(f\"Duration: {batch['end_times'][i] - batch['start_times'][i]:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "**Foundation for Production-Ready Multimodal AI Systems**\n",
    "\n",
    "This comprehensive exploration of dataset loading and preprocessing pipelines provides the essential practical skills needed for real-world multimodal AI development:\n",
    "\n",
    "1. **Production-Ready Data Handling**\n",
    "   - Understanding how to work with large-scale, real-world multimodal datasets\n",
    "   - Implementing robust data loading pipelines that handle edge cases and errors gracefully\n",
    "\n",
    "3. **Multimodal-Specific Considerations**\n",
    "   - Text preprocessing techniques optimized for multimodal alignment and integration\n",
    "   - Cross-modal consistency preservation throughout the preprocessing pipeline\n",
    "   - Augmentation strategies that enhance robustness while maintaining semantic coherence\n",
    "\n",
    "4. **Engineering Best Practices**\n",
    "   - Modular design patterns enabling easy extension and maintenance\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs_w7_s1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
