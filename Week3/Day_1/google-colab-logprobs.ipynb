{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba455e5",
   "metadata": {},
   "source": [
    "IMPORTANT: This notebook is meant to be run on Google Collab, will not work for you on local unless you have powerful Nvidia GPU\n",
    "\n",
    "https://colab.research.google.com/drive/185RaaIFspi9x2EYM42CUbrWjAYha1ynZ?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800043a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed48ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"auto\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Can you do another chat completions call but this time — show log probabilities of top 10 tokens for each token inferred. And then the final message\n",
    "\n",
    "import math, pandas as pd\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")\n",
    "# print(completion.choices[0].message)\n",
    "# completion.choices[0].logprobs\n",
    "\n",
    "logprobs = completion.choices[0].logprobs     # shortcut\n",
    "records = []\n",
    "for idx, (tok, lp, top) in enumerate(zip(\n",
    "        logprobs.tokens,\n",
    "        logprobs.token_logprobs,\n",
    "        logprobs.top_logprobs)):\n",
    "\n",
    "    chosen = f\"{tok} ({math.exp(lp):.3f})\"\n",
    "    alts = sorted(top.items(), key=lambda kv: kv[1], reverse=True)[:5]\n",
    "    alts_fmt = [f\"{t} ({math.exp(lp_):.3f})\" for t, lp_ in alts]\n",
    "    alts_fmt += [''] * (5 - len(alts_fmt))          # pad to 5\n",
    "\n",
    "    records.append([idx, chosen, *alts_fmt])\n",
    "\n",
    "df = pd.DataFrame(records,\n",
    "                  columns=[\"Idx\", \"Chosen (p)\", \"Alt-1\", \"Alt-2\",\n",
    "                           \"Alt-3\", \"Alt-4\", \"Alt-5\"])\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc489b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Do same question \"What is capital of france\" with chat completion — allow controlling other factors like output tokens, temperature\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=10,  # Set the maximum number of output tokens\n",
    "    temperature=2 # Set the temperature for creativity (0.0 to 1.0)\n",
    ")\n",
    "\n",
    "completion.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
