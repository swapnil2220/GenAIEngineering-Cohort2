{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Embeddings Tutorial with Hugging Face\n",
    "=================================================\n",
    "  \n",
    "[View on Google Colab](https://colab.research.google.com/drive/1mClrNFwUeztQjUL4NXZslr9nEoeD67sj?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191.25s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196.76s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0.0 in /Users/srishtigureja/Desktop/Personal/outskill/.venv/lib/python3.13/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch transformers\n",
    "! pip install \"numpy<2.0.0\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_embeddings(\n",
    "    texts: Union[str, List[str]], \n",
    "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    max_length: int = 512,\n",
    "    normalize: bool = True,\n",
    "    device: Optional[str] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create dense vector embeddings for text using pre-trained transformer models.\n",
    "    \n",
    "    This function uses Hugging Face transformers to convert text into numerical\n",
    "    representations that capture semantic meaning. These embeddings can be used\n",
    "    for similarity search, clustering, classification, and other NLP tasks.\n",
    "    \n",
    "    Args:\n",
    "        texts (Union[str, List[str]]): Single text string or list of text strings\n",
    "            to embed. Each text will be converted to a fixed-size vector.\n",
    "        model_name (str, optional): Name of the pre-trained model from Hugging Face.\n",
    "            Default is \"sentence-transformers/all-MiniLM-L6-v2\" which is optimized\n",
    "            for sentence-level embeddings. Other options include:\n",
    "            - \"sentence-transformers/all-mpnet-base-v2\" (higher quality, slower)\n",
    "            - \"bert-base-uncased\" (general BERT model)\n",
    "            - \"distilbert-base-uncased\" (faster, smaller BERT variant)\n",
    "        max_length (int, optional): Maximum sequence length for tokenization.\n",
    "            Longer texts will be truncated. Default is 512 tokens.\n",
    "        normalize (bool, optional): Whether to normalize embeddings to unit vectors.\n",
    "            Normalized embeddings work better for cosine similarity. Default is True.\n",
    "        device (Optional[str], optional): Device to run the model on ('cpu', 'cuda').\n",
    "            If None, automatically detects available device.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings with shape (n_texts, embedding_dim).\n",
    "            Each row represents the embedding vector for one input text.\n",
    "    \n",
    "    Example:\n",
    "        >>> # Single text embedding\n",
    "        >>> text = \"This is a sample sentence for embedding.\"\n",
    "        >>> embedding = create_text_embeddings(text)\n",
    "        >>> print(f\"Embedding shape: {embedding.shape}\")\n",
    "        \n",
    "        >>> # Multiple texts\n",
    "        >>> texts = [\"Hello world\", \"Natural language processing\", \"Machine learning\"]\n",
    "        >>> embeddings = create_text_embeddings(texts)\n",
    "        >>> print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Handle input format - convert single string to list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "        single_input = True\n",
    "    else:\n",
    "        single_input = False\n",
    "    \n",
    "    # Step 2: Determine compute device (GPU if available, else CPU)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Step 3: Load pre-trained tokenizer and model\n",
    "    # The tokenizer converts text to tokens (numbers) that the model can process\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # The model creates contextual embeddings from the tokenized input\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()  # Set to evaluation mode (disables dropout, etc.)\n",
    "    \n",
    "    # Step 4: Tokenize all input texts\n",
    "    # This converts text to token IDs and creates attention masks\n",
    "    print(f\"Tokenizing {len(texts)} text(s)...\")\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,          # Pad shorter sequences to match longest\n",
    "        truncation=True,       # Truncate sequences longer than max_length\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"    # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # Move tokenized inputs to the same device as the model\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "    \n",
    "    # Step 5: Generate embeddings using the model\n",
    "    print(\"Generating embeddings...\")\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        # Forward pass through the transformer model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden states (contextual embeddings for each token)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Step 6: Pool token embeddings to create sentence-level embeddings\n",
    "        # Method: Mean pooling with attention mask weighting\n",
    "        # This averages token embeddings while ignoring padding tokens\n",
    "        \n",
    "        # Expand attention mask to match hidden state dimensions\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "        \n",
    "        # Apply attention mask and compute mean\n",
    "        sum_embeddings = torch.sum(last_hidden_states * attention_mask_expanded, 1)\n",
    "        sum_mask = torch.sum(attention_mask_expanded, 1)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        \n",
    "        # Mean pooling: divide sum by count of non-padding tokens\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    # Step 7: Move embeddings back to CPU and convert to numpy\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "    \n",
    "    # Step 8: Optional normalization for better similarity computation\n",
    "    if normalize:\n",
    "        # L2 normalization: each embedding becomes a unit vector\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings = embeddings / (norms + 1e-9)  # Avoid division by zero\n",
    "    \n",
    "    print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Step 9: Return single embedding if single input was provided\n",
    "    if single_input:\n",
    "        return embeddings[0]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_text_embeddings():\n",
    "    \"\"\"\n",
    "    Demonstrate the text embedding function with various examples.\n",
    "    \n",
    "    This function shows practical usage scenarios and helps verify\n",
    "    that the embedding function works correctly.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEXT EMBEDDINGS DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Example 1: Single text embedding\n",
    "    print(\"\\n1. Single Text Embedding:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    single_text = \"Artificial intelligence is transforming the world.\"\n",
    "    embedding = create_text_embeddings(single_text)\n",
    "    \n",
    "    print(f\"Input text: '{single_text}'\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"Embedding magnitude: {np.linalg.norm(embedding):.4f}\")\n",
    "    print(f\"First 5 dimensions: {embedding[:5]}\")\n",
    "    \n",
    "    # Example 2: Multiple texts with similarity comparison\n",
    "    print(\"\\n\\n2. Multiple Texts with Similarity Analysis:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    texts = [\n",
    "        \"Machine learning is a subset of artificial intelligence.\",\n",
    "        \"Deep learning uses neural networks with multiple layers.\",\n",
    "        \"I love eating pizza and pasta for dinner.\",\n",
    "        \"Natural language processing helps computers understand text.\",\n",
    "        \"My favorite Italian food is definitely pizza.\"\n",
    "    ]\n",
    "    \n",
    "    embeddings = create_text_embeddings(texts)\n",
    "    \n",
    "    print(f\"Input texts: {len(texts)} sentences\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = np.dot(embeddings, embeddings.T)\n",
    "    \n",
    "    print(\"\\nCosine Similarity Matrix:\")\n",
    "    print(\"(Higher values = more similar texts)\")\n",
    "    \n",
    "    # Print similarity matrix with text indices\n",
    "    print(\"\\nText Index Reference:\")\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"{i}: {text[:50]}...\")\n",
    "    \n",
    "    print(f\"\\nSimilarity Matrix:\")\n",
    "    print(\"     \", end=\"\")\n",
    "    for j in range(len(texts)):\n",
    "        print(f\"{j:6}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        print(f\"{i}: \", end=\"\")\n",
    "        for j in range(len(texts)):\n",
    "            print(f\"{similarity_matrix[i,j]:6.3f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Find most similar pair (excluding diagonal)\n",
    "    max_sim = 0\n",
    "    max_pair = (0, 0)\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i+1, len(texts)):\n",
    "            if similarity_matrix[i,j] > max_sim:\n",
    "                max_sim = similarity_matrix[i,j]\n",
    "                max_pair = (i, j)\n",
    "    \n",
    "    print(f\"\\nMost similar texts (similarity: {max_sim:.3f}):\")\n",
    "    print(f\"Text {max_pair[0]}: {texts[max_pair[0]]}\")\n",
    "    print(f\"Text {max_pair[1]}: {texts[max_pair[1]]}\")\n",
    "    \n",
    "    # Example 3: Different model comparison\n",
    "    print(\"\\n\\n3. Comparing Different Models:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    \n",
    "    models_to_test = [\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",  # Fast, good quality\n",
    "        \"distilbert-base-uncased\"                   # General BERT variant\n",
    "    ]\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        print(f\"\\nTesting model: {model_name}\")\n",
    "        embedding = create_text_embeddings(test_text, model_name=model_name)\n",
    "        print(f\"Embedding dimension: {embedding.shape[0]}\")\n",
    "        print(f\"Sample values: {embedding[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEXT EMBEDDINGS DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "1. Single Text Embedding:\n",
      "------------------------------\n",
      "Using device: cpu\n",
      "Loading model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 1 text(s)...\n",
      "Generating embeddings...\n",
      "Generated embeddings with shape: (1, 384)\n",
      "Input text: 'Artificial intelligence is transforming the world.'\n",
      "Embedding shape: (384,)\n",
      "Embedding magnitude: 1.0000\n",
      "First 5 dimensions: [ 0.03872417 -0.00110556  0.08271619 -0.01628861  0.0465431 ]\n",
      "\n",
      "\n",
      "2. Multiple Texts with Similarity Analysis:\n",
      "---------------------------------------------\n",
      "Using device: cpu\n",
      "Loading model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Tokenizing 5 text(s)...\n",
      "Generating embeddings...\n",
      "Generated embeddings with shape: (5, 384)\n",
      "Input texts: 5 sentences\n",
      "Embeddings shape: (5, 384)\n",
      "\n",
      "Cosine Similarity Matrix:\n",
      "(Higher values = more similar texts)\n",
      "\n",
      "Text Index Reference:\n",
      "0: Machine learning is a subset of artificial intelli...\n",
      "1: Deep learning uses neural networks with multiple l...\n",
      "2: I love eating pizza and pasta for dinner....\n",
      "3: Natural language processing helps computers unders...\n",
      "4: My favorite Italian food is definitely pizza....\n",
      "\n",
      "Similarity Matrix:\n",
      "          0     1     2     3     4\n",
      "0:  1.000 0.489 0.062 0.388 0.090\n",
      "1:  0.489 1.000 0.011 0.308 0.037\n",
      "2:  0.062 0.011 1.000 0.076 0.686\n",
      "3:  0.388 0.308 0.076 1.000 0.010\n",
      "4:  0.090 0.037 0.686 0.010 1.000\n",
      "\n",
      "Most similar texts (similarity: 0.686):\n",
      "Text 2: I love eating pizza and pasta for dinner.\n",
      "Text 4: My favorite Italian food is definitely pizza.\n",
      "\n",
      "\n",
      "3. Comparing Different Models:\n",
      "-----------------------------------\n",
      "\n",
      "Testing model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Using device: cpu\n",
      "Loading model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Tokenizing 1 text(s)...\n",
      "Generating embeddings...\n",
      "Generated embeddings with shape: (1, 384)\n",
      "Embedding dimension: 384\n",
      "Sample values: [0.04393346 0.05893438 0.04817837]\n",
      "\n",
      "Testing model: distilbert-base-uncased\n",
      "Using device: cpu\n",
      "Loading model: distilbert-base-uncased\n",
      "Tokenizing 1 text(s)...\n",
      "Generating embeddings...\n",
      "Generated embeddings with shape: (1, 768)\n",
      "Embedding dimension: 768\n",
      "Sample values: [-0.01186158 -0.00205346 -0.00344327]\n"
     ]
    }
   ],
   "source": [
    "demonstrate_text_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
