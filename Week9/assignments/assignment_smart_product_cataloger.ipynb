{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Product Cataloger - Assignment\n",
    "\n",
    "[View on Google Colab](https://colab.research.google.com/drive/1fplzzeYAi-bo1oRrwC2hGnVN_OlbQysR?usp=sharing)\n",
    "\n",
    "Week 8: Multimodal AI for E-commerce Product Analysis\n",
    "\n",
    "OBJECTIVE: Build an AI system that can automatically analyze product images\n",
    "and generate metadata for e-commerce listings using CLIP and BLIP models.\n",
    "\n",
    "LEARNING GOALS:\n",
    "- Use CLIP for zero-shot image classification\n",
    "- Use BLIP for image captioning and visual question answering\n",
    "- Combine multiple AI models for practical applications\n",
    "- Build a complete product analysis pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering,\n",
    "    pipeline\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to store models (we'll load them once)\n",
    "clip_model = None\n",
    "clip_processor = None\n",
    "blip_caption_model = None\n",
    "blip_caption_processor = None\n",
    "blip_vqa_model = None\n",
    "blip_vqa_processor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Models from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    Load all required models for product analysis\n",
    "    \n",
    "    TODO: Load the following models and store them in global variables:\n",
    "    1. CLIP model for image classification\n",
    "    2. BLIP model for image captioning  \n",
    "    3. BLIP model for visual question answering\n",
    "    \n",
    "    MODELS TO LOAD:\n",
    "    - CLIP: \"openai/clip-vit-base-patch32\"\n",
    "    - BLIP Caption: \"Salesforce/blip-image-captioning-base\"\n",
    "    - BLIP VQA: \"Salesforce/blip-vqa-base\"\n",
    "    \n",
    "    HINT: Use the model loading patterns from Week8/session_2 notebooks\n",
    "    HINT: Use 'global' keyword to modify global variables\n",
    "    \n",
    "    EXAMPLE:\n",
    "    global clip_model, clip_processor\n",
    "    clip_model = ?\n",
    "    clip_processor = ?\n",
    "    \"\"\"\n",
    "    global clip_model, clip_processor, blip_caption_model, blip_caption_processor, blip_vqa_model, blip_vqa_processor\n",
    "    \n",
    "    print(\"üöÄ Loading models for Smart Product Cataloger...\")\n",
    "    \n",
    "    # TODO: Load CLIP model and processor\n",
    "    # clip_model = ?\n",
    "    # clip_processor = ?\n",
    "    \n",
    "    # TODO: Load BLIP caption model and processor\n",
    "    # blip_caption_model = ?\n",
    "    # blip_caption_processor = ?\n",
    "    \n",
    "    # TODO: Load BLIP VQA model and processor\n",
    "    # blip_vqa_model = ?\n",
    "    # blip_vqa_processor = ?\n",
    "    \n",
    "    # DUMMY IMPLEMENTATION (Remove this when implementing)\n",
    "    print(\"‚ö†Ô∏è DUMMY: Models not loaded yet - implement the TODO sections above\")\n",
    "    \n",
    "    print(\"‚úÖ All models loaded successfully!\")\n",
    "\n",
    "# TEST: Load models\n",
    "print(\"üîß TESTING: Loading models...\")\n",
    "load_models()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image from URL\n",
    "  \n",
    "You can extend this function to load an image from a path on your local system and apply the required transforms to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load an image from a URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the image to load\n",
    "        \n",
    "    Returns:\n",
    "        Image.Image: PIL Image object or None if failed\n",
    "        \n",
    "    TODO: Implement image loading from URL\n",
    "    HINT: Use requests.get() and Image.open()\n",
    "    \n",
    "    EXAMPLE INPUT: \"https://images.unsplash.com/photo-1542291026-7eec264c27ff\"\n",
    "    EXAMPLE OUTPUT: PIL Image object of Nike shoes\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement image loading\n",
    "    # 1. Use requests.get() to fetch the image\n",
    "    # 2. Use Image.open() to create PIL Image\n",
    "    # 3. Convert to RGB format\n",
    "    # 4. Handle errors gracefully\n",
    "    \n",
    "    # DUMMY IMPLEMENTATION (Remove this when implementing)\n",
    "    print(f\"‚ö†Ô∏è DUMMY: Would load image from {url}\")\n",
    "    return None\n",
    "\n",
    "# TEST: Load image\n",
    "print(\"üì∏ TESTING: Loading image from URL...\")\n",
    "sample_url = \"https://images.unsplash.com/photo-1542291026-7eec264c27ff\"  # Nike shoes\n",
    "image = load_image_from_url(sample_url)\n",
    "\n",
    "print(f\"Image loaded successfully: {image is not None}\")\n",
    "if image:\n",
    "    print(f\"Image size: {image.size}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Classification using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_product_image(image: Image.Image, candidate_labels: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Classify image using CLIP zero-shot classification\n",
    "    \n",
    "    Args:\n",
    "        image (Image.Image): PIL Image to classify\n",
    "        candidate_labels (List[str]): List of possible categories\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Classification results with labels and scores\n",
    "        \n",
    "    TODO: Implement zero-shot classification using CLIP\n",
    "    HINT: Use the pipeline approach from clip.ipynb\n",
    "    \n",
    "    EXAMPLE INPUT: \n",
    "        image = <PIL Image of shoes>\n",
    "        candidate_labels = [\"clothing\", \"shoes\", \"electronics\", \"furniture\"]\n",
    "        \n",
    "    EXAMPLE OUTPUT:\n",
    "        [\n",
    "            {\"label\": \"shoes\", \"score\": 0.8945},\n",
    "            {\"label\": \"clothing\", \"score\": 0.0823},\n",
    "            {\"label\": \"electronics\", \"score\": 0.0156},\n",
    "            {\"label\": \"furniture\", \"score\": 0.0076}\n",
    "        ]\n",
    "    \"\"\"\n",
    "    print(\"üîç Classifying product category...\")\n",
    "    \n",
    "    # TODO: Implement CLIP classification\n",
    "    # 1. Create a zero-shot-image-classification pipeline\n",
    "    # clip_pipeline = ?\n",
    "\n",
    "    # 2. Use the pipeline to classify the image\n",
    "    # results = ?\n",
    "\n",
    "    # 3. Return the results\n",
    "    # return results\n",
    "    \n",
    "    # DUMMY IMPLEMENTATION (Remove this when implementing)\n",
    "    dummy_results = [\n",
    "        {\"label\": candidate_labels[0], \"score\": 0.8945},\n",
    "        {\"label\": candidate_labels[1], \"score\": 0.0823},\n",
    "    ]\n",
    "    print(\"‚ö†Ô∏è DUMMY: Returning fake classification results\")\n",
    "    \n",
    "    return dummy_results\n",
    "\n",
    "# TEST: Classify image\n",
    "print(\"üîç TESTING: Classifying product image...\")\n",
    "categories = [\"clothing\", \"shoes\", \"electronics\", \"furniture\", \"books\", \"toys\"]\n",
    "classification_results = classify_product_image(image, categories)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "for result in classification_results:\n",
    "    print(f\"  {result['label']}: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Product Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_caption(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Generate a descriptive caption for the image using BLIP\n",
    "    \n",
    "    Args:\n",
    "        image (Image.Image): PIL Image to caption\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption describing the image\n",
    "        \n",
    "    TODO: Implement image captioning using BLIP\n",
    "    HINT: Use the captioning approach from blip.ipynb\n",
    "    HINT: Use the global blip_caption_model and blip_caption_processor\n",
    "    \n",
    "    EXAMPLE INPUT: <PIL Image of red Nike sneakers>\n",
    "    EXAMPLE OUTPUT: \"a pair of red nike sneakers on a white background\"\n",
    "    \"\"\"\n",
    "    print(\"üìù Generating image caption...\")\n",
    "    \n",
    "    # TODO: Implement BLIP captioning\n",
    "    # 1. Process the image using blip_caption_processor\n",
    "    # inputs = ?\n",
    "\n",
    "    # 2. Generate caption using blip_caption_model\n",
    "    # with torch.no_grad():\n",
    "    #     out = ?\n",
    "\n",
    "    # 3. Decode and return the caption\n",
    "    # caption = ?\n",
    "    # return caption\n",
    "    \n",
    "    # DUMMY IMPLEMENTATION (Remove this when implementing)\n",
    "    dummy_caption = \"a product on a white background\"\n",
    "    print(f\"‚ö†Ô∏è DUMMY: Generated caption: '{dummy_caption}'\")\n",
    "    return dummy_caption\n",
    "\n",
    "# TEST: Generate caption\n",
    "print(\"üìù TESTING: Generating product caption...\")\n",
    "caption = generate_product_caption(image)\n",
    "print(f\"Generated Caption: '{caption}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Question and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_about_product(image: Image.Image, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Answer questions about the image using BLIP VQA\n",
    "    \n",
    "    Args:\n",
    "        image (Image.Image): PIL Image to analyze\n",
    "        question (str): Question to ask about the image\n",
    "        \n",
    "    Returns:\n",
    "        str: Answer to the question\n",
    "        \n",
    "    TODO: Implement visual question answering using BLIP VQA\n",
    "    HINT: Use the VQA approach from blip.ipynb\n",
    "    HINT: Use the global blip_vqa_model and blip_vqa_processor\n",
    "    \n",
    "    EXAMPLE INPUT: \n",
    "        image = <PIL Image of red shoes>\n",
    "        question = \"What color are these shoes?\"\n",
    "        \n",
    "    EXAMPLE OUTPUT: \"red\"\n",
    "    \"\"\"\n",
    "    print(f\"‚ùì Answering: '{question}'\")\n",
    "    \n",
    "    # TODO: Implement BLIP VQA\n",
    "    # 1. Process image and question using blip_vqa_processor\n",
    "    # inputs = ?\n",
    "\n",
    "    # 2. Generate answer using blip_vqa_model\n",
    "    # with torch.no_grad():\n",
    "    #     out = ?\n",
    "\n",
    "    # 3. Decode and return the answer\n",
    "    # answer = ?\n",
    "    # return answer\n",
    "    \n",
    "    # DUMMY IMPLEMENTATION (Remove this when implementing)\n",
    "    dummy_answer = \"unknown\"\n",
    "    print(f\"‚ö†Ô∏è DUMMY: Answer: {dummy_answer}\")\n",
    "    return dummy_answer\n",
    "\n",
    "# TEST: Visual Question Answering\n",
    "print(\"‚ùì TESTING: Visual Question Answering...\")\n",
    "test_questions = [\n",
    "    \"What color are these shoes?\",\n",
    "    \"What brand are these shoes?\",\n",
    "    \"Are these sneakers or dress shoes?\"\n",
    "]\n",
    "\n",
    "print(\"VQA Results:\")\n",
    "for question in test_questions:\n",
    "    answer = ask_about_product(image, question)\n",
    "    print(f\"  Q: {question}\")\n",
    "    print(f\"  A: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Category Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_questions(category: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate relevant questions based on product category\n",
    "    \n",
    "    Args:\n",
    "        category (str): Product category (e.g., \"shoes\", \"clothing\")\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of relevant questions for the category\n",
    "        \n",
    "    TODO: Create category-specific questions for better product analysis\n",
    "    \n",
    "    EXAMPLE INPUT: \"shoes\"\n",
    "    EXAMPLE OUTPUT: [\n",
    "        \"What color are these shoes?\",\n",
    "        \"What type of shoes are these?\", \n",
    "        \"What brand are these shoes?\",\n",
    "        \"What material are these shoes made of?\"\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create a mapping of categories to relevant questions\n",
    "    # Categories to support: clothing, shoes, electronics, furniture, books, toys\n",
    "    \n",
    "    question_map = {\n",
    "        \"shoes\": [\n",
    "            \"What color are these shoes?\",\n",
    "            \"What type of shoes are these?\",\n",
    "            \"What brand are these shoes?\",\n",
    "            \"What material are these shoes made of?\"\n",
    "        ],\n",
    "        \"clothing\": [\n",
    "            # TODO: Add clothing-specific questions\n",
    "        ],\n",
    "        \"electronics\": [\n",
    "            # TODO: Add electronics-specific questions  \n",
    "        ],\n",
    "        \"furniture\": [\n",
    "            # TODO: Add furniture-specific questions\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # TODO: Return questions for the category, or default questions if category not found\n",
    "    return question_map.get(category, [\n",
    "        \"What color is this?\",\n",
    "        \"What type of item is this?\",\n",
    "        \"What is this made of?\"\n",
    "    ])\n",
    "\n",
    "# TEST: Category questions\n",
    "print(\"üìã TESTING: Category-specific questions...\")\n",
    "test_categories = [\"shoes\", \"clothing\", \"electronics\", \"furniture\"]\n",
    "\n",
    "for category in test_categories:\n",
    "    questions = get_category_questions(category)\n",
    "    print(f\"{category.title()} Questions:\")\n",
    "    for q in questions:\n",
    "        print(f\"  - {q}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_product(image_url_or_pil: Union[str, Image.Image]) -> Dict:\n",
    "    \"\"\"\n",
    "    Main function to analyze a product image and generate complete metadata\n",
    "    \n",
    "    Args:\n",
    "        image_url_or_pil (Union[str, Image.Image]): Image URL or PIL Image\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Complete product analysis including category, description, and attributes\n",
    "        \n",
    "    TODO: Implement the complete product analysis pipeline\n",
    "    \n",
    "    PIPELINE STEPS:\n",
    "    1. Load image (if URL provided)\n",
    "    2. Classify product category using CLIP\n",
    "    3. Generate product description using BLIP captioning\n",
    "    4. Ask category-specific questions using BLIP VQA\n",
    "    5. Compile and return results\n",
    "    \n",
    "    EXAMPLE INPUT: \"https://images.unsplash.com/photo-1542291026-7eec264c27ff\"\n",
    "    EXAMPLE OUTPUT: {\n",
    "        \"category\": {\"name\": \"shoes\", \"confidence\": 0.8945},\n",
    "        \"description\": \"a pair of red nike sneakers on a white background\",\n",
    "        \"attributes\": {\n",
    "            \"What color are these shoes?\": \"red\",\n",
    "            \"What type of shoes are these?\": \"sneakers\",\n",
    "            \"What brand are these shoes?\": \"nike\"\n",
    "        },\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting product analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # TODO: Step 1 - Load image if URL provided\n",
    "        # if isinstance(image_url_or_pil, str):\n",
    "        #     image = load_image_from_url(image_url_or_pil)\n",
    "        #     if image is None:\n",
    "        #         return {\"error\": \"Failed to load image\", \"status\": \"failed\"}\n",
    "        # else:\n",
    "        #     image = image_url_or_pil\n",
    "        \n",
    "        # TODO: Step 2 - Classify product category\n",
    "        # product_categories = [\"clothing\", \"shoes\", \"electronics\", \"furniture\", \"books\", \"toys\"]\n",
    "        # classification_results = classify_product_image(image, product_categories)\n",
    "        # top_category = classification_results[0]\n",
    "        \n",
    "        # TODO: Step 3 - Generate product description\n",
    "        # description = generate_product_caption(image)\n",
    "        \n",
    "        # TODO: Step 4 - Get category-specific questions and ask them\n",
    "        # category = top_category['label']\n",
    "        # questions = get_category_questions(category)\n",
    "        # qa_results = {}\n",
    "        # for question in questions:\n",
    "        #     answer = ask_about_product(image, question)\n",
    "        #     qa_results[question] = answer\n",
    "        \n",
    "        # TODO: Step 5 - Compile results\n",
    "        # result = {\n",
    "        #     \"category\": {\"name\": category, \"confidence\": top_category['score']},\n",
    "        #     \"description\": description,\n",
    "        #     \"attributes\": qa_results,\n",
    "        #     \"status\": \"success\"\n",
    "        # }\n",
    "        \n",
    "        # DUMMY IMPLEMENTATION (Remove this when implementing)\n",
    "        result = {\n",
    "            \"category\": {\"name\": \"shoes\", \"confidence\": 0.8945},\n",
    "            \"description\": \"a pair of red sneakers on a white background\",\n",
    "            \"attributes\": {\n",
    "                \"What color are these shoes?\": \"red\",\n",
    "                \"What type of shoes are these?\": \"sneakers\"\n",
    "            },\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        print(\"‚ö†Ô∏è DUMMY: Returning fake analysis results\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Product analysis complete!\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during processing: {e}\")\n",
    "        return {\"error\": str(e), \"status\": \"failed\"}\n",
    "\n",
    "# TEST: Complete product analysis\n",
    "print(\"üöÄ TESTING: Complete product analysis pipeline...\")\n",
    "analysis_result = analyze_product(sample_url)\n",
    "print(\"Complete Analysis Result:\")\n",
    "print(analysis_result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs_w7_s1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
