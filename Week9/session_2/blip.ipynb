{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP Tutorial using HuggingFace\n",
    "\n",
    "[View on Google Colab](https://colab.research.google.com/drive/1UsXI3Er7aWsL3pIDsFJ_FCJMlUnkdmTa?usp=sharing)\n",
    "\n",
    "Contents Covered:\n",
    "1. Loading BLIP Models using HuggingFace\n",
    "2. Load Sample Image\n",
    "3. Image Caption Generation\n",
    "4. Visual Quesiton Answering\n",
    "5. Image Text Matching\n",
    "6. BLIP with HuggingFace Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers pillow matplotlib\n",
    "# !pip install \"numpy<2.0.0\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration,\n",
    "    BlipForQuestionAnswering,\n",
    "    BlipForImageTextRetrieval,\n",
    "    pipeline\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import List, Union, Tuple, Optional\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blip_model(\n",
    "    model_name: str = \"Salesforce/blip-image-captioning-base\",\n",
    "    task: str = \"captioning\"\n",
    ") -> Tuple[Union[BlipForConditionalGeneration, BlipForQuestionAnswering, BlipForImageTextRetrieval], BlipProcessor]:\n",
    "    \"\"\"\n",
    "    Load BLIP model and processor from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the BLIP model to load\n",
    "        task (str): Task type - 'captioning', 'vqa', or 'retrieval'\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: Loaded model and processor\n",
    "    \"\"\"\n",
    "    print(f\"Loading BLIP model: {model_name}\")\n",
    "    print(f\"Task: {task}\")\n",
    "    \n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    if task == \"captioning\":\n",
    "        model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
    "    elif task == \"vqa\":\n",
    "        model = BlipForQuestionAnswering.from_pretrained(model_name)\n",
    "    elif task == \"retrieval\":\n",
    "        model = BlipForImageTextRetrieval.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task: {task}\")\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Model config: {model.config}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# Execute: Load BLIP model for captioning\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING BLIP MODEL FOR CAPTIONING\")\n",
    "print(\"=\" * 60)\n",
    "captioning_model, captioning_processor = load_blip_model(\n",
    "    \"Salesforce/blip-image-captioning-base\", \n",
    "    \"captioning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load an image from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the image\n",
    "        \n",
    "    Returns:\n",
    "        Image.Image: PIL Image object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(response.raw)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from URL: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Execute: Load sample image\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING SAMPLE IMAGE\")\n",
    "print(\"=\" * 60)\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "sample_image = load_image_from_url(image_url)\n",
    "print(f\"Image loaded from: {image_url}\")\n",
    "print(f\"Image size: {sample_image.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(\n",
    "    image: Union[str, Image.Image],\n",
    "    model: BlipForConditionalGeneration,\n",
    "    processor: BlipProcessor,\n",
    "    max_length: int = 50,\n",
    "    num_beams: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate caption for an image using BLIP.\n",
    "    \n",
    "    Args:\n",
    "        image (Union[str, Image.Image]): Image URL or PIL Image\n",
    "        model (BlipForConditionalGeneration): BLIP model\n",
    "        processor (BlipProcessor): BLIP processor\n",
    "        max_length (int): Maximum caption length\n",
    "        num_beams (int): Number of beams for beam search\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    print(\"Generating image caption...\")\n",
    "    \n",
    "    # Load image if URL provided\n",
    "    if isinstance(image, str):\n",
    "        image = load_image_from_url(image)\n",
    "    \n",
    "    # Process image\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode caption\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Generated Caption: '{caption}'\")\n",
    "    return caption\n",
    "\n",
    "\n",
    "# Execute: Generate image caption\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMAGE CAPTIONING\")\n",
    "print(\"=\" * 60)\n",
    "caption = generate_image_caption(sample_image, captioning_model, captioning_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_image_captioning(\n",
    "    image: Union[str, Image.Image],\n",
    "    text_prompt: str,\n",
    "    model: BlipForConditionalGeneration,\n",
    "    processor: BlipProcessor,\n",
    "    max_length: int = 50,\n",
    "    num_beams: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate conditional caption for an image with text prompt using BLIP.\n",
    "    \n",
    "    Args:\n",
    "        image (Union[str, Image.Image]): Image URL or PIL Image\n",
    "        text_prompt (str): Text prompt to condition the caption\n",
    "        model (BlipForConditionalGeneration): BLIP model\n",
    "        processor (BlipProcessor): BLIP processor\n",
    "        max_length (int): Maximum caption length\n",
    "        num_beams (int): Number of beams for beam search\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated conditional caption\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating conditional caption with prompt: '{text_prompt}'\")\n",
    "    \n",
    "    # Load image if URL provided\n",
    "    if isinstance(image, str):\n",
    "        image = load_image_from_url(image)\n",
    "    \n",
    "    # Process image and text\n",
    "    inputs = processor(image, text_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode caption\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Generated Conditional Caption: '{caption}'\")\n",
    "    return caption\n",
    "\n",
    "\n",
    "# Execute: Conditional image captioning\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONDITIONAL IMAGE CAPTIONING\")\n",
    "print(\"=\" * 60)\n",
    "text_prompts = [\n",
    "    \"a picture of\",\n",
    "    \"this is\",\n",
    "    \"I can see\"\n",
    "]\n",
    "\n",
    "conditional_captions = []\n",
    "for prompt in text_prompts:\n",
    "    conditional_caption = conditional_image_captioning(\n",
    "        sample_image, prompt, captioning_model, captioning_processor\n",
    "    )\n",
    "    conditional_captions.append(conditional_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blip_vqa_model(model_name: str = \"Salesforce/blip-vqa-base\") -> Tuple[BlipForQuestionAnswering, BlipProcessor]:\n",
    "    \"\"\"\n",
    "    Load BLIP model specifically for Visual Question Answering.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the BLIP VQA model\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: VQA model and processor\n",
    "    \"\"\"\n",
    "    print(f\"Loading BLIP VQA model: {model_name}\")\n",
    "    \n",
    "    model = BlipForQuestionAnswering.from_pretrained(model_name)\n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"VQA model loaded successfully!\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# Execute: Load VQA model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING BLIP VQA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "vqa_model, vqa_processor = load_blip_vqa_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_question_answering(\n",
    "    image: Union[str, Image.Image],\n",
    "    question: str,\n",
    "    model: BlipForQuestionAnswering,\n",
    "    processor: BlipProcessor,\n",
    "    max_length: int = 20\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Answer questions about an image using BLIP VQA.\n",
    "    \n",
    "    Args:\n",
    "        image (Union[str, Image.Image]): Image URL or PIL Image\n",
    "        question (str): Question about the image\n",
    "        model (BlipForQuestionAnswering): BLIP VQA model\n",
    "        processor (BlipProcessor): BLIP processor\n",
    "        max_length (int): Maximum answer length\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated answer\n",
    "    \"\"\"\n",
    "    print(f\"Answering question: '{question}'\")\n",
    "    \n",
    "    # Load image if URL provided\n",
    "    if isinstance(image, str):\n",
    "        image = load_image_from_url(image)\n",
    "    \n",
    "    # Process image and question\n",
    "    inputs = processor(image, question, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode answer\n",
    "    answer = processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Answer: '{answer}'\")\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Execute: Visual Question Answering\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUAL QUESTION ANSWERING\")\n",
    "print(\"=\" * 60)\n",
    "questions = [\n",
    "    \"What animals are in the image?\",\n",
    "    \"How many cats are there?\",\n",
    "    \"What are the cats doing?\",\n",
    "    \"What color are the cats?\",\n",
    "    \"Where are the cats located?\"\n",
    "]\n",
    "\n",
    "answers = []\n",
    "for question in questions:\n",
    "    answer = visual_question_answering(sample_image, question, vqa_model, vqa_processor)\n",
    "    answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Text Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blip_retrieval_model(model_name: str = \"Salesforce/blip-itm-base-coco\") -> Tuple[BlipForImageTextRetrieval, BlipProcessor]:\n",
    "    \"\"\"\n",
    "    Load BLIP model for Image-Text Matching/Retrieval.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the BLIP ITM model\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: ITM model and processor\n",
    "    \"\"\"\n",
    "    print(f\"Loading BLIP ITM model: {model_name}\")\n",
    "    \n",
    "    model = BlipForImageTextRetrieval.from_pretrained(model_name)\n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"ITM model loaded successfully!\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# Execute: Load ITM model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING BLIP IMAGE-TEXT MATCHING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "itm_model, itm_processor = load_blip_retrieval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_text_matching(\n",
    "    image: Union[str, Image.Image],\n",
    "    texts: List[str],\n",
    "    model: BlipForImageTextRetrieval,\n",
    "    processor: BlipProcessor\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute image-text matching scores using BLIP ITM.\n",
    "    \n",
    "    Args:\n",
    "        image (Union[str, Image.Image]): Image URL or PIL Image\n",
    "        texts (List[str]): List of text descriptions\n",
    "        model (BlipForImageTextRetrieval): BLIP ITM model\n",
    "        processor (BlipProcessor): BLIP processor\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Matching scores for each text\n",
    "    \"\"\"\n",
    "    print(\"Computing image-text matching scores...\")\n",
    "    \n",
    "    # Load image if URL provided\n",
    "    if isinstance(image, str):\n",
    "        image = load_image_from_url(image)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Process image and text\n",
    "        inputs = processor(image, text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get matching score\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # ITM score (higher means better match)\n",
    "            itm_score = torch.nn.functional.softmax(outputs.itm_score, dim=1)\n",
    "            # Get the probability of match (index 1)\n",
    "            match_prob = itm_score[0][1].item()\n",
    "            scores.append(match_prob)\n",
    "    \n",
    "    print(\"Image-Text Matching Scores:\")\n",
    "    for i, (text, score) in enumerate(zip(texts, scores)):\n",
    "        print(f\"  '{text}': {score:.4f}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# Execute: Image-text matching\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMAGE-TEXT MATCHING\")\n",
    "print(\"=\" * 60)\n",
    "text_descriptions = [\n",
    "    \"two cats lying on a couch\",\n",
    "    \"dogs playing in a park\",\n",
    "    \"a car driving on a road\", \n",
    "    \"cats sleeping together\",\n",
    "    \"animals resting indoors\",\n",
    "    \"people walking on the street\"\n",
    "]\n",
    "matching_scores = image_text_matching(sample_image, text_descriptions, itm_model, itm_processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIP with HuggingFace Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_pipeline_demo(image: Union[str, Image.Image]):\n",
    "    \"\"\"\n",
    "    Demonstrate BLIP using Hugging Face pipelines.\n",
    "    \n",
    "    Args:\n",
    "        image (Union[str, Image.Image]): Image to process\n",
    "    \"\"\"\n",
    "    print(\"Using BLIP with Hugging Face Pipelines...\")\n",
    "    \n",
    "    # Image captioning pipeline\n",
    "    print(\"\\n1. Image Captioning Pipeline:\")\n",
    "    captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
    "    caption_result = captioner(image)\n",
    "    print(f\"Pipeline Caption: {caption_result[0]['generated_text']}\")\n",
    "    \n",
    "    # Visual Question Answering pipeline\n",
    "    print(\"\\n2. Visual Question Answering Pipeline:\")\n",
    "    vqa_pipeline = pipeline(\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\n",
    "    \n",
    "    sample_questions = [\n",
    "        \"What animals are shown?\",\n",
    "        \"How many animals are there?\",\n",
    "        \"What are they doing?\"\n",
    "    ]\n",
    "    \n",
    "    for question in sample_questions:\n",
    "        result = vqa_pipeline(image=image, question=question)\n",
    "        # VQA pipeline returns a list, so we need to access the first element\n",
    "        if isinstance(result, list):\n",
    "            result = result[0]\n",
    "        print(f\"Q: {question}\")\n",
    "        # Check if score is available, otherwise just show the answer\n",
    "        if 'score' in result:\n",
    "            print(f\"A: {result['answer']} (score: {result['score']:.4f})\")\n",
    "        else:\n",
    "            print(f\"A: {result['answer']}\")\n",
    "            print(f\"Available keys in result: {list(result.keys())}\")\n",
    "\n",
    "\n",
    "# Execute: Pipeline demo\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BLIP PIPELINE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "blip_pipeline_demo(sample_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_blip_results(\n",
    "    image: Image.Image,\n",
    "    caption: str,\n",
    "    vqa_pairs: List[Tuple[str, str]],\n",
    "    itm_scores: List[Tuple[str, float]],\n",
    "    title: str = \"BLIP Model Results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize BLIP results with image, caption, VQA, and ITM scores.\n",
    "    \n",
    "    Args:\n",
    "        image (Image.Image): Input image\n",
    "        caption (str): Generated caption\n",
    "        vqa_pairs (List[Tuple[str, str]]): Question-answer pairs\n",
    "        itm_scores (List[Tuple[str, float]]): Text descriptions and matching scores\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Create grid layout\n",
    "    gs = fig.add_gridspec(2, 3, height_ratios=[1, 1], width_ratios=[2, 1, 1])\n",
    "    \n",
    "    # Display image\n",
    "    ax1 = fig.add_subplot(gs[:, 0])\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(f'Input Image\\nCaption: \"{caption}\"', fontsize=12, pad=20)\n",
    "    \n",
    "    # Display VQA results\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Visual Q&A Results', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    vqa_text = \"\"\n",
    "    for i, (q, a) in enumerate(vqa_pairs[:5]):  # Show top 5\n",
    "        vqa_text += f\"Q: {q}\\nA: {a}\\n\\n\"\n",
    "    \n",
    "    ax2.text(0.05, 0.95, vqa_text, transform=ax2.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace')\n",
    "    \n",
    "    # Display ITM scores\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    texts, scores = zip(*itm_scores)\n",
    "    y_pos = np.arange(len(texts))\n",
    "    bars = ax3.barh(y_pos, scores)\n",
    "    ax3.set_yticks(y_pos)\n",
    "    ax3.set_yticklabels([t[:20] + '...' if len(t) > 20 else t for t in texts], fontsize=8)\n",
    "    ax3.set_xlabel('Matching Score')\n",
    "    ax3.set_title('Image-Text Matching', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax3.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "    \n",
    "    # Add model comparison info\n",
    "    ax4 = fig.add_subplot(gs[1, 1:])\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('BLIP Model Capabilities', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    capabilities_text = \"\"\"\n",
    "BLIP (Bootstrapping Language-Image Pre-training) Capabilities:\n",
    "\n",
    "• Image Captioning: Generate natural language descriptions of images\n",
    "• Conditional Captioning: Generate captions conditioned on text prompts  \n",
    "• Visual Question Answering: Answer questions about image content\n",
    "• Image-Text Matching: Compute similarity between images and text\n",
    "• Zero-shot Transfer: Apply to new tasks without fine-tuning\n",
    "\n",
    "Models Used:\n",
    "• Captioning: Salesforce/blip-image-captioning-base\n",
    "• VQA: Salesforce/blip-vqa-base  \n",
    "• ITM: Salesforce/blip-itm-base-coco\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.05, 0.95, capabilities_text, transform=ax4.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Execute: Visualization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data for visualization\n",
    "vqa_pairs = list(zip(questions, answers))\n",
    "itm_pairs = list(zip(text_descriptions, matching_scores))\n",
    "\n",
    "visualize_blip_results(\n",
    "    sample_image, \n",
    "    caption, \n",
    "    vqa_pairs, \n",
    "    itm_pairs,\n",
    "    \"BLIP Tutorial: Comprehensive Results\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BLIP TUTORIAL COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey BLIP capabilities demonstrated:\")\n",
    "print(\"• Image Captioning (unconditional and conditional)\")\n",
    "print(\"• Visual Question Answering\")\n",
    "print(\"• Image-Text Matching/Retrieval\")\n",
    "print(\"• Pipeline-based inference\")\n",
    "print(\"• Comprehensive visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
