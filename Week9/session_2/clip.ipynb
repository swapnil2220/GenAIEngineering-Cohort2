{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Tutorial Using HuggingFace\n",
    "\n",
    "[View on Google Colab](https://colab.research.google.com/drive/1tpVJFdg5_7_k-Bsthcap3MNADgFiqHtx?usp=sharing)\n",
    "\n",
    "Contents Covered: \n",
    "1. Loading CLIP Models from HuggingFace\n",
    "2. Load Sample Image from URL\n",
    "3. Zero Shot Image Classification\n",
    "4. Compute Image Text Similarity\n",
    "5. Image Features Extraction\n",
    "6. Text Features Extraction\n",
    "7. Similarity Matrix Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers pillow matplotlib\n",
    "# !pip install \"numpy<2.0.0\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    CLIPProcessor, \n",
    "    CLIPModel, \n",
    "    CLIPTokenizer, \n",
    "    CLIPImageProcessor,\n",
    "    pipeline\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import List, Union, Tuple\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CLIP Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model_name: str = \"openai/clip-vit-base-patch32\") -> Tuple[CLIPModel, CLIPProcessor]:\n",
    "    \"\"\"\n",
    "    Load CLIP model and processor from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the CLIP model to load\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[CLIPModel, CLIPProcessor]: Loaded model and processor\n",
    "    \"\"\"\n",
    "    print(f\"Loading CLIP model: {model_name}\")\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Vision config: {model.config.vision_config}\")\n",
    "    print(f\"Text config: {model.config.text_config}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# Execute: Load CLIP model\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING CLIP MODEL\")\n",
    "print(\"=\" * 60)\n",
    "model, processor = load_clip_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Image from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load an image from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the image\n",
    "        \n",
    "    Returns:\n",
    "        Image.Image: PIL Image object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(response.raw)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from URL: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Execute: Load sample image\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING SAMPLE IMAGE\")\n",
    "print(\"=\" * 60)\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "sample_image = load_image_from_url(image_url)\n",
    "print(f\"Image loaded from: {image_url}\")\n",
    "print(f\"Image size: {sample_image.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_image_classification(\n",
    "    image: Union[str, Image.Image], \n",
    "    candidate_labels: List[str],\n",
    "    model_name: str = \"openai/clip-vit-base-patch32\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Perform zero-shot image classification using CLIP pipeline.\n",
    "    \n",
    "    Args:\n",
    "        image (Union[str, Image.Image]): Image URL or PIL Image\n",
    "        candidate_labels (List[str]): List of possible labels\n",
    "        model_name (str): CLIP model name\n",
    "        \n",
    "    Returns:\n",
    "        dict: Classification results with scores\n",
    "    \"\"\"\n",
    "    print(\"Performing zero-shot image classification...\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    clip_pipeline = pipeline(\n",
    "        task=\"zero-shot-image-classification\",\n",
    "        model=model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    # Perform classification\n",
    "    results = clip_pipeline(image, candidate_labels=candidate_labels)\n",
    "    \n",
    "    print(\"Classification Results:\")\n",
    "    for result in results:\n",
    "        print(f\"  {result['label']}: {result['score']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Execute: Zero-shot classification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT IMAGE CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a bird\"]\n",
    "classification_results = zero_shot_image_classification(sample_image, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Image Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_text_similarity(\n",
    "    image: Union[str, Image.Image],\n",
    "    texts: List[str],\n",
    "    model: CLIPModel,\n",
    "    processor: CLIPProcessor\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute similarity scores between an image and multiple text descriptions.\n",
    "    \n",
    "    Args:\n",
    "        image (Union[str, Image.Image]): Image URL or PIL Image\n",
    "        texts (List[str]): List of text descriptions\n",
    "        model (CLIPModel): CLIP model\n",
    "        processor (CLIPProcessor): CLIP processor\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Similarity scores\n",
    "    \"\"\"\n",
    "    print(\"Computing image-text similarities...\")\n",
    "    \n",
    "    # Load image if URL provided\n",
    "    if isinstance(image, str):\n",
    "        image = load_image_from_url(image)\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Get the logits (similarity scores)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    \n",
    "    print(\"Similarity Scores:\")\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"  '{text}': {probs[0][i].item():.4f}\")\n",
    "    \n",
    "    return probs.numpy()\n",
    "\n",
    "\n",
    "# Execute: Image-text similarity\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMAGE-TEXT SIMILARITY\")\n",
    "print(\"=\" * 60)\n",
    "text_descriptions = [\n",
    "    \"two cats lying on a couch\",\n",
    "    \"dogs playing in a park\", \n",
    "    \"a car driving on a road\",\n",
    "    \"cats sleeping together\",\n",
    "    \"animals resting indoors\"\n",
    "]\n",
    "similarities = compute_image_text_similarity(sample_image, text_descriptions, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(\n",
    "    image: Image.Image,\n",
    "    texts: List[str],\n",
    "    similarities: np.ndarray,\n",
    "    title: str = \"CLIP Image-Text Similarities\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize CLIP results with image and similarity scores.\n",
    "    \n",
    "    Args:\n",
    "        image (Image.Image): Input image\n",
    "        texts (List[str]): Text descriptions\n",
    "        similarities (np.ndarray): Similarity scores\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Display image\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Input Image')\n",
    "    \n",
    "    # Display similarity scores\n",
    "    y_pos = np.arange(len(texts))\n",
    "    bars = ax2.barh(y_pos, similarities[0])\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(texts)\n",
    "    ax2.set_xlabel('Similarity Score')\n",
    "    ax2.set_title('Text Similarities')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Execute: Visualize results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "visualize_results(sample_image, text_descriptions, similarities, \n",
    "                 \"CLIP Tutorial: Image-Text Similarities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(\n",
    "    images: Union[List[Image.Image], List[str]],\n",
    "    model: CLIPModel,\n",
    "    processor: CLIPProcessor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract image features using CLIP vision encoder.\n",
    "    \n",
    "    Args:\n",
    "        images (Union[List[Image.Image], List[str]]): List of images or URLs\n",
    "        model (CLIPModel): CLIP model\n",
    "        processor (CLIPProcessor): CLIP processor\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Image features\n",
    "    \"\"\"\n",
    "    print(f\"Extracting features for {len(images)} images...\")\n",
    "    \n",
    "    # Load images if URLs provided\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        if isinstance(img, str):\n",
    "            processed_images.append(load_image_from_url(img))\n",
    "        else:\n",
    "            processed_images.append(img)\n",
    "    \n",
    "    # Process images\n",
    "    inputs = processor(images=processed_images, return_tensors=\"pt\")\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        \n",
    "    # Normalize features\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    print(f\"Extracted features shape: {image_features.shape}\")\n",
    "    return image_features\n",
    "\n",
    "\n",
    "# Execute: Extract image features\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMAGE FEATURE EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "image_features = extract_image_features([sample_image], model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(\n",
    "    texts: List[str],\n",
    "    model: CLIPModel,\n",
    "    processor: CLIPProcessor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract text features using CLIP text encoder.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of text descriptions\n",
    "        model (CLIPModel): CLIP model\n",
    "        processor (CLIPProcessor): CLIP processor\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Text features\n",
    "    \"\"\"\n",
    "    print(f\"Extracting features for {len(texts)} texts...\")\n",
    "    \n",
    "    # Process texts\n",
    "    inputs = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "        \n",
    "    # Normalize features\n",
    "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    print(f\"Extracted features shape: {text_features.shape}\")\n",
    "    return text_features\n",
    "\n",
    "\n",
    "# Execute: Extract text features\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEXT FEATURE EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "text_features = extract_text_features(text_descriptions, model, processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(\n",
    "    image_features: torch.Tensor,\n",
    "    text_features: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute similarity matrix between image and text features.\n",
    "    \n",
    "    Args:\n",
    "        image_features (torch.Tensor): Image features\n",
    "        text_features (torch.Tensor): Text features\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Similarity matrix\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = torch.matmul(image_features, text_features.T)\n",
    "    \n",
    "    print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "# Execute: Compute similarity matrix\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SIMILARITY MATRIX COMPUTATION\")\n",
    "print(\"=\" * 60)\n",
    "similarity_matrix = compute_similarity_matrix(image_features, text_features)\n",
    "print(f\"Similarity scores: {similarity_matrix[0].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
