{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio Embeddings Tutorial with Hugging Face\n",
    "===========================================\n",
    "  \n",
    "[View on Google Colab](https://colab.research.google.com/drive/1ExzturxhZDXktxqSiMWxv0cSvwQ8-mVt?usp=sharing)\n",
    "  \n",
    "This tutorial demonstrates how to create embeddings for audio using:\n",
    "- Pre-trained audio models from Hugging Face\n",
    "- Speech Commands dataset for demonstration\n",
    "- Visualization of latent space embeddings\n",
    "- Audio similarity analysis\n",
    "\n",
    "Includes detailed explanations and working examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers datasets matplotlib seaborn librosa scikit-learn ipython\n",
    "# !pip install \"numpy<2.0.0\"\n",
    "# !pip install --upgrade scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoModel, Wav2Vec2Processor, Wav2Vec2Model\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Speech Commands Dataset from HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_speech_commands_dataset(num_samples: int = 1000) -> tuple:\n",
    "    \"\"\"\n",
    "    Load Speech Commands dataset from Hugging Face datasets.\n",
    "    \n",
    "    Args:\n",
    "        num_samples (int, optional): Number of samples to load from the dataset.\n",
    "            Default is 1000 for faster processing.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (audio_arrays, labels, sampling_rates) where audio_arrays is a list of \n",
    "               numpy arrays, labels is a list of corresponding command labels, and\n",
    "               sampling_rates is a list of sampling rates.\n",
    "    \"\"\"\n",
    "    print(f\"Loading Speech Commands dataset from Hugging Face...\")\n",
    "    \n",
    "    # Load the Speech Commands dataset from Hugging Face\n",
    "    dataset = load_dataset(\"speech_commands\", \"v0.01\", split=\"train\", trust_remote_code=True)\n",
    "    \n",
    "    # Take a subset for demonstration\n",
    "    dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    \n",
    "    # Extract audio data, labels, and sampling rates\n",
    "    audio_arrays = []\n",
    "    labels = []\n",
    "    sampling_rates = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        audio_arrays.append(sample[\"audio\"][\"array\"])\n",
    "        labels.append(sample[\"label\"])\n",
    "        sampling_rates.append(sample[\"audio\"][\"sampling_rate\"])\n",
    "    \n",
    "    # Get label names\n",
    "    label_names = dataset.features[\"label\"].names\n",
    "    \n",
    "    print(f\"Loaded {len(audio_arrays)} audio samples\")\n",
    "    print(f\"Sampling rate: {sampling_rates[0]}Hz\")\n",
    "    print(f\"Available commands: {label_names}\")\n",
    "    \n",
    "    return audio_arrays, labels, sampling_rates, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Speech Commands dataset\n",
    "print(\"\\n1. Loading Speech Commands Dataset:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "audio_arrays, labels, sampling_rates, label_names = load_speech_commands_dataset(num_samples=1000)\n",
    "print(\"Unique labels after filtering:\", set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_two_commands(command_names=['bed', 'cat'], min_samples_per_class=50):\n",
    "    \"\"\"\n",
    "    Load the Speech Commands dataset and filter for two commands, ensuring both are present.\n",
    "\n",
    "    Args:\n",
    "        command_names (list): List of two command names to filter.\n",
    "        min_samples_per_class (int): Minimum number of samples per class to collect.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (audio_arrays, labels, sampling_rates, label_names)\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    # Load a large chunk of the dataset\n",
    "    dataset = load_dataset(\"speech_commands\", \"v0.01\", split=\"train\")\n",
    "    label_names = dataset.features[\"label\"].names\n",
    "    command_indices = [label_names.index(cmd) for cmd in command_names]\n",
    "\n",
    "    audio_arrays, labels, sampling_rates = [], [], []\n",
    "    class_counts = [0, 0]\n",
    "\n",
    "    for sample in dataset:\n",
    "        label = sample[\"label\"]\n",
    "        if label in command_indices:\n",
    "            idx = command_indices.index(label)\n",
    "            if class_counts[idx] < min_samples_per_class:\n",
    "                audio_arrays.append(sample[\"audio\"][\"array\"])\n",
    "                labels.append(idx)  # Relabel as 0 or 1\n",
    "                sampling_rates.append(sample[\"audio\"][\"sampling_rate\"])\n",
    "                class_counts[idx] += 1\n",
    "            if all(count >= min_samples_per_class for count in class_counts):\n",
    "                break\n",
    "\n",
    "    filtered_label_names = command_names\n",
    "    print(f\"Loaded {class_counts[0]} samples for '{command_names[0]}' and {class_counts[1]} samples for '{command_names[1]}'\")\n",
    "    return audio_arrays, labels, sampling_rates, filtered_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_arrays, labels, sampling_rates, label_names = load_and_filter_two_commands(['bed', 'stop'], min_samples_per_class=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Audio Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio_samples(audio_arrays, labels, label_names, sampling_rates, num_samples=4):\n",
    "    \"\"\"\n",
    "    Visualize waveforms and spectrograms of sample audio files, with play buttons.\n",
    "    Ensures both classes are represented in the plot.\n",
    "\n",
    "    Args:\n",
    "        audio_arrays (List[np.ndarray]): List of audio waveforms.\n",
    "        labels (List[int]): List of corresponding labels.\n",
    "        label_names (List[str]): List of label names.\n",
    "        sampling_rates (List[int]): List of sampling rates.\n",
    "        num_samples (int, optional): Total number of samples to visualize (default: 4).\n",
    "    \"\"\"\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    samples_per_class = max(1, num_samples // len(unique_labels))\n",
    "    indices = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_indices = np.where(np.array(labels) == label)[0]\n",
    "        chosen = np.random.choice(label_indices, min(samples_per_class, len(label_indices)), replace=False)\n",
    "        indices.extend(chosen)\n",
    "\n",
    "    # If not enough, fill up to num_samples\n",
    "    if len(indices) < num_samples:\n",
    "        remaining = list(set(range(len(labels))) - set(indices))\n",
    "        np.random.shuffle(remaining)\n",
    "        indices.extend(remaining[:num_samples - len(indices)])\n",
    "\n",
    "    # Plotting\n",
    "    cols = 2\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "\n",
    "    for i, idx in enumerate(indices[:num_samples]):\n",
    "        ax = axes[i]\n",
    "        time = np.linspace(0, len(audio_arrays[idx]) / sampling_rates[idx], len(audio_arrays[idx]))\n",
    "        ax.plot(time, audio_arrays[idx])\n",
    "        ax.set_title(f'Command: {label_names[labels[idx]]}')\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    for i in range(num_samples, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    plt.suptitle('Sample Audio Waveforms from Speech Commands Dataset', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Display audio players below the plots\n",
    "    for idx in indices[:num_samples]:\n",
    "        display(Audio(audio_arrays[idx], rate=sampling_rates[idx], autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize sample audio waveforms\n",
    "print(\"\\n2. Sample Audio Waveforms:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "visualize_audio_samples(audio_arrays, labels, label_names, sampling_rates, num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the audio embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audio_embeddings(\n",
    "    audio_arrays: List[np.ndarray],\n",
    "    sampling_rates: List[int],\n",
    "    model_name: str = \"facebook/wav2vec2-base\",\n",
    "    normalize: bool = True,\n",
    "    device: Optional[str] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create dense vector embeddings for audio using pre-trained transformer models.\n",
    "    \n",
    "    This function uses Hugging Face audio models to convert audio waveforms into numerical\n",
    "    representations that capture acoustic and semantic features. These embeddings can be used\n",
    "    for audio similarity search, clustering, classification, and retrieval tasks.\n",
    "    \n",
    "    Args:\n",
    "        audio_arrays (List[np.ndarray]): List of audio waveforms as numpy arrays.\n",
    "        sampling_rates (List[int]): List of corresponding sampling rates for each audio.\n",
    "        model_name (str, optional): Name of the pre-trained audio model from Hugging Face.\n",
    "            Default is \"facebook/wav2vec2-base\". Other options include:\n",
    "            - \"facebook/wav2vec2-large\" (larger model, better quality)\n",
    "            - \"facebook/hubert-base-ls960\" (HuBERT model)\n",
    "            - \"microsoft/wavlm-base\" (WavLM model)\n",
    "        normalize (bool, optional): Whether to normalize embeddings to unit vectors.\n",
    "            Normalized embeddings work better for cosine similarity. Default is True.\n",
    "        device (Optional[str], optional): Device to run the model on ('cpu', 'cuda').\n",
    "            If None, automatically detects available device.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings with shape (n_audio, embedding_dim).\n",
    "            Each row represents the embedding vector for one input audio.\n",
    "    \n",
    "    Example:\n",
    "        >>> # Single audio embedding\n",
    "        >>> audio, sr = librosa.load(\"path/to/audio.wav\", sr=16000)\n",
    "        >>> embedding = create_audio_embeddings([audio], [sr])\n",
    "        >>> print(f\"Embedding shape: {embedding.shape}\")\n",
    "        \n",
    "        >>> # Multiple audio files\n",
    "        >>> embeddings = create_audio_embeddings(audio_list, sr_list)\n",
    "        >>> print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Determine compute device (GPU if available, else CPU)\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Loading audio model: {model_name}\")\n",
    "    \n",
    "    # Step 2: Load pre-trained processor and model\n",
    "    # The processor handles audio preprocessing (resampling, normalization, etc.)\n",
    "    try:\n",
    "        if \"wav2vec2\" in model_name.lower():\n",
    "            processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "            model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "        else:\n",
    "            # Fallback to generic feature extractor\n",
    "            processor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "            model = AutoModel.from_pretrained(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Falling back to wav2vec2-base...\")\n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Step 3: Process audio data\n",
    "    print(f\"Processing {len(audio_arrays)} audio sample(s)...\")\n",
    "    \n",
    "    # Resample all audio to the model's expected sampling rate (usually 16kHz)\n",
    "    target_sr = processor.sampling_rate if hasattr(processor, 'sampling_rate') else 16000\n",
    "    processed_audios = []\n",
    "    \n",
    "    for i, (audio, sr) in enumerate(zip(audio_arrays, sampling_rates)):\n",
    "        # Resample if necessary\n",
    "        if sr != target_sr:\n",
    "            print(f\"  Resampling audio {i+1} from {sr}Hz to {target_sr}Hz...\")\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        # Ensure audio is 1D\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = audio.mean(axis=0)  # Convert stereo to mono\n",
    "        \n",
    "        processed_audios.append(audio)\n",
    "    \n",
    "    # Step 4: Extract features using the processor\n",
    "    print(\"Extracting audio features...\")\n",
    "    \n",
    "    # Process audio in batches for efficiency\n",
    "    batch_size = 8\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(processed_audios), batch_size):\n",
    "        batch_audios = processed_audios[i:i+batch_size]\n",
    "        \n",
    "        # Process the batch\n",
    "        inputs = processor(\n",
    "            batch_audios,\n",
    "            sampling_rate=target_sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=100000  # Limit to ~6 seconds at 16kHz\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        input_values = inputs.input_values.to(device)\n",
    "        attention_mask = inputs.attention_mask.to(device) if 'attention_mask' in inputs else None\n",
    "        \n",
    "        # Step 5: Generate embeddings using the model\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            if attention_mask is not None:\n",
    "                outputs = model(input_values, attention_mask=attention_mask)\n",
    "            else:\n",
    "                outputs = model(input_values)\n",
    "            \n",
    "            # Extract embeddings based on model architecture\n",
    "            if hasattr(outputs, 'last_hidden_state'):\n",
    "                # For Wav2Vec2 and similar models\n",
    "                # Use mean pooling over the sequence dimension\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "                \n",
    "                if attention_mask is not None:\n",
    "                    # Apply attention mask for proper mean pooling\n",
    "                    mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                    sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)\n",
    "                    sum_mask = torch.sum(mask_expanded, 1)\n",
    "                    embeddings = sum_embeddings / torch.clamp(sum_mask, min=1e-9)\n",
    "                else:\n",
    "                    # Simple mean pooling\n",
    "                    embeddings = hidden_states.mean(dim=1)\n",
    "            \n",
    "            elif hasattr(outputs, 'pooler_output'):\n",
    "                # For models with pooler output\n",
    "                embeddings = outputs.pooler_output\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Could not extract embeddings from model outputs\")\n",
    "        \n",
    "        # Move to CPU and store\n",
    "        batch_embeddings = embeddings.cpu().numpy()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Step 6: Concatenate all batches\n",
    "    embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    \n",
    "    # Step 7: Optional normalization for better similarity computation\n",
    "    if normalize:\n",
    "        # L2 normalization: each embedding becomes a unit vector\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings = embeddings / (norms + 1e-9)  # Avoid division by zero\n",
    "    \n",
    "    print(f\"Generated audio embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create audio embeddings\n",
    "print(\"\\n3. Creating Audio Embeddings:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Use a smaller subset for faster processing\n",
    "subset_size = 200\n",
    "subset_audios = audio_arrays[:subset_size]\n",
    "subset_labels = labels[:subset_size]\n",
    "subset_srs = sampling_rates[:subset_size]\n",
    "\n",
    "embeddings = create_audio_embeddings(subset_audios, subset_srs)\n",
    "\n",
    "print(f\"Created embeddings for {len(subset_audios)} audio samples\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the PCA and t-SNE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio_embeddings_2d(embeddings, labels, label_names, method=\"tsne\", title=\"Audio Embeddings\"):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional audio embeddings in 2D space using dimensionality reduction.\n",
    "    Uses blue for 'bed' and red for 'cat' for clear distinction.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): High-dimensional embeddings with shape (n_samples, embedding_dim).\n",
    "        labels (List[int]): List of labels corresponding to each embedding.\n",
    "        label_names (List[str]): List of label names.\n",
    "        method (str, optional): Dimensionality reduction method ('pca' or 'tsne'). Default is 'tsne'.\n",
    "        title (str, optional): Title for the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Reducing dimensionality using {method.upper()}...\")\n",
    "\n",
    "    if method.lower() == \"pca\":\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "        embeddings_2d = reducer.fit_transform(embeddings)\n",
    "    elif method.lower() == \"tsne\":\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "        embeddings_2d = reducer.fit_transform(embeddings)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'tsne'\")\n",
    "\n",
    "    # Manual color map for two classes\n",
    "    color_map = {0: 'blue', 1: 'red'}\n",
    "    label_color_map = {name: color_map[i] for i, name in enumerate(label_names)}\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, label in enumerate(sorted(set(labels))):\n",
    "        mask = np.array(labels) == label\n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "            c=label_color_map[label_names[label]],\n",
    "            label=label_names[label],\n",
    "            alpha=0.7, s=20\n",
    "        )\n",
    "\n",
    "    plt.title(f'{title} - {method.upper()} Visualization')\n",
    "    plt.xlabel(f'{method.upper()} Component 1')\n",
    "    plt.ylabel(f'{method.upper()} Component 2')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return embeddings_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize embeddings in 2D\n",
    "print(\"\\n4. Visualizing Embeddings in 2D Space:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# PCA visualization\n",
    "print(\"Creating PCA visualization...\")\n",
    "pca_embeddings = visualize_audio_embeddings_2d(embeddings, subset_labels, label_names,\n",
    "                                                method=\"pca\", \n",
    "                                                title=\"Speech Commands Embeddings\")\n",
    "\n",
    "# t-SNE visualization\n",
    "print(\"Creating t-SNE visualization...\")\n",
    "tsne_embeddings = visualize_audio_embeddings_2d(embeddings, subset_labels, label_names,\n",
    "                                                method=\"tsne\", \n",
    "                                                title=\"Speech Commands Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Cosine Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_audio_similarity_matrix(embeddings: np.ndarray, \n",
    "                                  labels: List[int], \n",
    "                                  label_names: List[str],\n",
    "                                  max_samples: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute and visualize similarity matrix for a subset of audio embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): Array of embeddings.\n",
    "        labels (List[int]): List of corresponding labels.\n",
    "        label_names (List[str]): List of label names.\n",
    "        max_samples (int, optional): Maximum number of samples to include in similarity matrix.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Similarity matrix.\n",
    "    \"\"\"\n",
    "    # Take a subset for visualization\n",
    "    if len(embeddings) > max_samples:\n",
    "        indices = np.random.choice(len(embeddings), max_samples, replace=False)\n",
    "        subset_embeddings = embeddings[indices]\n",
    "        subset_labels = [labels[i] for i in indices]\n",
    "    else:\n",
    "        subset_embeddings = embeddings\n",
    "        subset_labels = labels\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = np.dot(subset_embeddings, subset_embeddings.T)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Sort by labels for better visualization\n",
    "    sorted_indices = np.argsort(subset_labels)\n",
    "    sorted_similarity = similarity_matrix[sorted_indices][:, sorted_indices]\n",
    "    sorted_labels = [label_names[subset_labels[i]] for i in sorted_indices]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(sorted_similarity, \n",
    "                xticklabels=sorted_labels, \n",
    "                yticklabels=sorted_labels,\n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Cosine Similarity'})\n",
    "    \n",
    "    plt.title('Cosine Similarity Matrix of Audio Embeddings\\n(sorted by command labels)')\n",
    "    plt.xlabel('Audio Sample (by command)')\n",
    "    plt.ylabel('Audio Sample (by command)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compute similarity matrix\n",
    "print(\"\\n5. Computing Similarity Matrix:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "similarity_matrix = compute_audio_similarity_matrix(embeddings, subset_labels, label_names,\n",
    "                                                    max_samples=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Quality of the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Analyze embedding quality\n",
    "print(\"\\n6. Embedding Quality Analysis:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Compute average intra-class vs inter-class similarity\n",
    "intra_similarities = []\n",
    "inter_similarities = []\n",
    "\n",
    "for i in range(len(subset_labels)):\n",
    "    for j in range(i+1, len(subset_labels)):\n",
    "        similarity = np.dot(embeddings[i], embeddings[j])\n",
    "        \n",
    "        if subset_labels[i] == subset_labels[j]:\n",
    "            intra_similarities.append(similarity)\n",
    "        else:\n",
    "            inter_similarities.append(similarity)\n",
    "\n",
    "print(f\"Average intra-class similarity: {np.mean(intra_similarities):.4f}\")\n",
    "print(f\"Average inter-class similarity: {np.mean(inter_similarities):.4f}\")\n",
    "print(f\"Separation ratio: {np.mean(intra_similarities) / np.mean(inter_similarities):.4f}\")\n",
    "\n",
    "# Plot similarity distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(intra_similarities, bins=50, alpha=0.7, label='Intra-class (Same Command)', density=True)\n",
    "plt.hist(inter_similarities, bins=50, alpha=0.7, label='Inter-class (Different Commands)', density=True)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Intra-class vs Inter-class Similarities')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Command-wise analysis\n",
    "print(\"\\n7. Command-wise Embedding Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Analyze average embeddings per command\n",
    "unique_labels = sorted(set(subset_labels))\n",
    "command_embeddings = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    mask = np.array(subset_labels) == label\n",
    "    if np.any(mask):\n",
    "        avg_embedding = embeddings[mask].mean(axis=0)\n",
    "        command_embeddings.append(avg_embedding)\n",
    "        print(f\"Command '{label_names[label]}': {np.sum(mask)} samples, \"\n",
    "                f\"avg embedding norm: {np.linalg.norm(avg_embedding):.4f}\")\n",
    "\n",
    "# Compute command-to-command similarity\n",
    "command_embeddings = np.array(command_embeddings)\n",
    "command_similarity = np.dot(command_embeddings, command_embeddings.T)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "command_names = [label_names[label] for label in unique_labels]\n",
    "sns.heatmap(command_similarity,\n",
    "            xticklabels=command_names,\n",
    "            yticklabels=command_names,\n",
    "            annot=True,\n",
    "            fmt='.3f',\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            square=True)\n",
    "plt.title('Average Command-to-Command Similarity')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
