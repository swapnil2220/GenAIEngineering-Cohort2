{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Product Cataloger - Solution\n",
    "\n",
    "[View on Google Colab](https://colab.research.google.com/drive/1xNdJcdzUSNV3Uae0KRqaevSzIm5PXA3k?usp=sharing)\n",
    "\n",
    "Week 8: Multimodal AI for E-commerce Product Analysis\n",
    "\n",
    "This is the complete solution showing how to build an AI system that can \n",
    "automatically analyze product images and generate metadata for e-commerce \n",
    "listings using CLIP and BLIP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering,\n",
    "    pipeline\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Analyze Food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Analyze Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to store models (we'll load them once)\n",
    "clip_model = None\n",
    "clip_processor = None\n",
    "blip_caption_model = None\n",
    "blip_caption_processor = None\n",
    "blip_vqa_model = None\n",
    "blip_vqa_processor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Models from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TESTING: Loading models...\n",
      "üöÄ Loading models for Smart Product Cataloger...\n",
      "üì¶ Loading CLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading BLIP caption model...\n",
      "üì¶ Loading BLIP VQA model...\n",
      "‚úÖ All models loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    Load all required models for product analysis\n",
    "    \n",
    "    SOLUTION: We load three different models:\n",
    "    1. CLIP for zero-shot image classification\n",
    "    2. BLIP for image captioning\n",
    "    3. BLIP for visual question answering\n",
    "    \n",
    "    We use global variables to store the models so they're loaded once\n",
    "    and can be reused across all function calls.\n",
    "    \"\"\"\n",
    "    global clip_model, clip_processor, blip_caption_model, blip_caption_processor, blip_vqa_model, blip_vqa_processor\n",
    "    \n",
    "    print(\"üöÄ Loading models for Smart Product Cataloger...\")\n",
    "    \n",
    "    # SOLUTION: Load CLIP model and processor for classification\n",
    "    print(\"üì¶ Loading CLIP model...\")\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # SOLUTION: Load BLIP caption model and processor for image captioning\n",
    "    print(\"üì¶ Loading BLIP caption model...\")\n",
    "    blip_caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    blip_caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    \n",
    "    # SOLUTION: Load BLIP VQA model and processor for question answering\n",
    "    print(\"üì¶ Loading BLIP VQA model...\")\n",
    "    blip_vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "    blip_vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "    \n",
    "    print(\"‚úÖ All models loaded successfully!\")\n",
    "\n",
    "# TEST: Load models\n",
    "print(\"üîß TESTING: Loading models...\")\n",
    "load_models()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image from URL\n",
    "  \n",
    "You can extend this function to load an image from a path on your local system and apply the required transforms to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ TESTING: Loading image from URL...\n",
      "Image loaded successfully: True\n",
      "Image size: (5472, 3648)\n"
     ]
    }
   ],
   "source": [
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load an image from a URL\n",
    "    \n",
    "    SOLUTION: We use the requests library to fetch the image data,\n",
    "    then PIL to open it and convert to RGB format. We handle errors\n",
    "    gracefully by returning None if something goes wrong.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SOLUTION: Implement image loading with error handling\n",
    "    try:\n",
    "        # 1. Use requests.get() to fetch the image with streaming\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        # 2. Use Image.open() to create PIL Image from the response\n",
    "        image = Image.open(response.raw)\n",
    "        \n",
    "        # 3. Convert to RGB format (ensures compatibility)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 4. Handle errors gracefully\n",
    "        print(f\"‚ùå Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "# TEST: Load image\n",
    "print(\"üì∏ TESTING: Loading image from URL...\")\n",
    "sample_url = \"https://images.unsplash.com/photo-1542291026-7eec264c27ff\"  # Nike shoes\n",
    "\n",
    "image = load_image_from_url(sample_url)\n",
    "print(f\"Image loaded successfully: {image is not None}\")\n",
    "\n",
    "if image:\n",
    "    print(f\"Image size: {image.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Classification using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TESTING: Classifying product image...\n",
      "üîç Classifying product category...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Results:\n",
      "  shoes: 0.9676\n",
      "  clothing: 0.0251\n",
      "  electronics: 0.0039\n",
      "  toys: 0.0013\n",
      "  furniture: 0.0011\n",
      "  books: 0.0009\n"
     ]
    }
   ],
   "source": [
    "def classify_product_image(image: Image.Image, candidate_labels: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Classify image using CLIP zero-shot classification\n",
    "    \n",
    "    SOLUTION: We use the transformers pipeline for zero-shot classification.\n",
    "    This is the easiest way to use CLIP - it handles all the preprocessing\n",
    "    and postprocessing for us. The pipeline returns results sorted by confidence.\n",
    "    \"\"\"\n",
    "    print(\"üîç Classifying product category...\")\n",
    "    \n",
    "    # SOLUTION: Use CLIP pipeline for zero-shot classification\n",
    "    # 1. Create a zero-shot-image-classification pipeline\n",
    "    clip_pipeline = pipeline(\n",
    "        task=\"zero-shot-image-classification\",\n",
    "        model=\"openai/clip-vit-base-patch32\"\n",
    "    )\n",
    "    \n",
    "    # 2. Use the pipeline to classify the image against candidate labels\n",
    "    results = clip_pipeline(image, candidate_labels=candidate_labels)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# TEST: Classify image\n",
    "print(\"üîç TESTING: Classifying product image...\")\n",
    "categories = [\"clothing\", \"shoes\", \"electronics\", \"furniture\", \"books\", \"toys\"]\n",
    "classification_results = classify_product_image(image, categories)\n",
    "print(\"Classification Results:\")\n",
    "for result in classification_results:\n",
    "    print(f\"  {result['label']}: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Product Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù TESTING: Generating product caption...\n",
      "üìù Generating image caption...\n",
      "Generated Caption: 'a red and white shoe on a red background'\n"
     ]
    }
   ],
   "source": [
    "def generate_product_caption(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Generate a descriptive caption for the image using BLIP\n",
    "    \n",
    "    SOLUTION: We use the BLIP captioning model to generate descriptions.\n",
    "    The process involves: preprocessing the image, generating tokens with\n",
    "    beam search, and decoding the tokens back to text.\n",
    "    \"\"\"\n",
    "    print(\"üìù Generating image caption...\")\n",
    "    \n",
    "    # SOLUTION: Use BLIP for image captioning\n",
    "    # 1. Process the image using blip_caption_processor\n",
    "    inputs = blip_caption_processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    # 2. Generate caption using blip_caption_model with beam search\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        out = blip_caption_model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,      # Maximum caption length\n",
    "            num_beams=5,        # Beam search for better quality\n",
    "            early_stopping=True # Stop when end token is generated\n",
    "        )\n",
    "    \n",
    "    # 3. Decode the generated tokens back to text\n",
    "    caption = blip_caption_processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# TEST: Generate caption\n",
    "print(\"üìù TESTING: Generating product caption...\")\n",
    "caption = generate_product_caption(image)\n",
    "print(f\"Generated Caption: '{caption}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Question and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì TESTING: Visual Question Answering...\n",
      "VQA Results:\n",
      "‚ùì Answering: 'What color are these shoes?'\n",
      "üí° Answer: red\n",
      "  Q: What color are these shoes?\n",
      "  A: red\n",
      "\n",
      "‚ùì Answering: 'What brand are these shoes?'\n",
      "üí° Answer: nike\n",
      "  Q: What brand are these shoes?\n",
      "  A: nike\n",
      "\n",
      "‚ùì Answering: 'Are these sneakers or dress shoes?'\n",
      "üí° Answer: sneakers\n",
      "  Q: Are these sneakers or dress shoes?\n",
      "  A: sneakers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ask_about_product(image: Image.Image, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Answer questions about the image using BLIP VQA\n",
    "    \n",
    "    SOLUTION: BLIP VQA takes both an image and a question as input,\n",
    "    then generates an answer. We process both inputs together,\n",
    "    generate answer tokens, and decode them to text.\n",
    "    \"\"\"\n",
    "    print(f\"‚ùì Answering: '{question}'\")\n",
    "    \n",
    "    # SOLUTION: Use BLIP VQA for visual question answering\n",
    "    # 1. Process image and question together using blip_vqa_processor\n",
    "    inputs = blip_vqa_processor(image, question, return_tensors=\"pt\")\n",
    "    \n",
    "    # 2. Generate answer using blip_vqa_model\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        out = blip_vqa_model.generate(\n",
    "            **inputs,\n",
    "            max_length=20,      # Answers are typically short\n",
    "            num_beams=5,        # Beam search for better quality\n",
    "            early_stopping=True # Stop when end token is generated\n",
    "        )\n",
    "    \n",
    "    # 3. Decode the generated tokens to get the answer\n",
    "    answer = blip_vqa_processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"üí° Answer: {answer}\")\n",
    "    return answer\n",
    "\n",
    "# TEST: Visual Question Answering\n",
    "print(\"‚ùì TESTING: Visual Question Answering...\")\n",
    "test_questions = [\n",
    "    \"What color are these shoes?\",\n",
    "    \"What brand are these shoes?\",\n",
    "    \"Are these sneakers or dress shoes?\"\n",
    "]\n",
    "\n",
    "print(\"VQA Results:\")\n",
    "for question in test_questions:\n",
    "    answer = ask_about_product(image, question)\n",
    "    print(f\"  Q: {question}\")\n",
    "    print(f\"  A: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Category Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã TESTING: Category-specific questions...\n",
      "Shoes Questions:\n",
      "  - What color are these shoes?\n",
      "  - What type of shoes are these?\n",
      "  - What brand are these shoes?\n",
      "  - What material are these shoes made of?\n",
      "  - Are these sneakers?\n",
      "\n",
      "Clothing Questions:\n",
      "  - What color is this clothing?\n",
      "  - What type of clothing is this?\n",
      "  - What material is this clothing made of?\n",
      "  - What size is this clothing?\n",
      "  - Is this formal or casual wear?\n",
      "\n",
      "Electronics Questions:\n",
      "  - What type of device is this?\n",
      "  - What brand is this device?\n",
      "  - What color is this device?\n",
      "  - Is this a smartphone or tablet?\n",
      "  - Does this have a screen?\n",
      "\n",
      "Furniture Questions:\n",
      "  - What type of furniture is this?\n",
      "  - What color is this furniture?\n",
      "  - What material is this furniture made of?\n",
      "  - Is this indoor or outdoor furniture?\n",
      "  - How many people can use this?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_category_questions(category: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate relevant questions based on product category\n",
    "    \n",
    "    SOLUTION: We create a mapping of product categories to relevant questions.\n",
    "    Each category has specific questions that help extract useful e-commerce\n",
    "    metadata. We provide default questions for unknown categories.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SOLUTION: Create comprehensive category-to-questions mapping\n",
    "    question_map = {\n",
    "        \"shoes\": [\n",
    "            \"What color are these shoes?\",\n",
    "            \"What type of shoes are these?\",\n",
    "            \"What brand are these shoes?\",\n",
    "            \"What material are these shoes made of?\",\n",
    "            \"Are these sneakers?\"\n",
    "        ],\n",
    "        \"clothing\": [\n",
    "            \"What color is this clothing?\",\n",
    "            \"What type of clothing is this?\",\n",
    "            \"What material is this clothing made of?\",\n",
    "            \"What size is this clothing?\",\n",
    "            \"Is this formal or casual wear?\"\n",
    "        ],\n",
    "        \"electronics\": [\n",
    "            \"What type of device is this?\",\n",
    "            \"What brand is this device?\",\n",
    "            \"What color is this device?\",\n",
    "            \"Is this a smartphone or tablet?\",\n",
    "            \"Does this have a screen?\"\n",
    "        ],\n",
    "        \"furniture\": [\n",
    "            \"What type of furniture is this?\",\n",
    "            \"What color is this furniture?\",\n",
    "            \"What material is this furniture made of?\",\n",
    "            \"Is this indoor or outdoor furniture?\",\n",
    "            \"How many people can use this?\"\n",
    "        ],\n",
    "        \"books\": [\n",
    "            \"What type of book is this?\",\n",
    "            \"What color is the book cover?\",\n",
    "            \"Is this a hardcover or paperback?\",\n",
    "            \"Does this book have text on the cover?\",\n",
    "            \"Is this a fiction or non-fiction book?\"\n",
    "        ],\n",
    "        \"toys\": [\n",
    "            \"What type of toy is this?\",\n",
    "            \"What color is this toy?\",\n",
    "            \"Is this toy for children or adults?\",\n",
    "            \"What material is this toy made of?\",\n",
    "            \"Is this an educational toy?\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # SOLUTION: Return questions for the category, or default questions\n",
    "    return question_map.get(category, [\n",
    "        \"What color is this?\",\n",
    "        \"What type of item is this?\",\n",
    "        \"What is this made of?\"\n",
    "    ])\n",
    "\n",
    "# TEST: Category questions\n",
    "print(\"üìã TESTING: Category-specific questions...\")\n",
    "test_categories = [\"shoes\", \"clothing\", \"electronics\", \"furniture\"]\n",
    "for category in test_categories:\n",
    "    questions = get_category_questions(category)\n",
    "    print(f\"{category.title()} Questions:\")\n",
    "    for q in questions:\n",
    "        print(f\"  - {q}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TESTING: Complete product analysis pipeline...\n",
      "üöÄ Starting product analysis...\n",
      "==================================================\n",
      "üîç Classifying product category...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generating image caption...\n",
      "‚ùì Answering: 'What color are these shoes?'\n",
      "üí° Answer: red\n",
      "‚ùì Answering: 'What type of shoes are these?'\n",
      "üí° Answer: nike\n",
      "‚ùì Answering: 'What brand are these shoes?'\n",
      "üí° Answer: nike\n",
      "‚ùì Answering: 'What material are these shoes made of?'\n",
      "üí° Answer: fabric\n",
      "‚ùì Answering: 'Are these sneakers?'\n",
      "üí° Answer: yes\n",
      "\n",
      "‚úÖ Product analysis complete!\n",
      "Complete Analysis Result:\n",
      "{'category': {'name': 'shoes', 'confidence': 0.9675810933113098}, 'description': 'a red and white shoe on a red background', 'attributes': {'What color are these shoes?': 'red', 'What type of shoes are these?': 'nike', 'What brand are these shoes?': 'nike', 'What material are these shoes made of?': 'fabric', 'Are these sneakers?': 'yes'}, 'status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "def analyze_product(image_url_or_pil: Union[str, Image.Image]) -> Dict:\n",
    "    \"\"\"\n",
    "    Main function to analyze a product image and generate complete metadata\n",
    "    \n",
    "    SOLUTION: This is the main pipeline that combines all our functions:\n",
    "    1. Load image (if URL provided)\n",
    "    2. Classify category using CLIP\n",
    "    3. Generate description using BLIP captioning\n",
    "    4. Ask category-specific questions using BLIP VQA\n",
    "    5. Compile everything into a structured result\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting product analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # SOLUTION: Step 1 - Load image if URL provided\n",
    "        if isinstance(image_url_or_pil, str):\n",
    "            image = load_image_from_url(image_url_or_pil)\n",
    "            if image is None:\n",
    "                return {\"error\": \"Failed to load image\", \"status\": \"failed\"}\n",
    "        else:\n",
    "            image = image_url_or_pil\n",
    "        \n",
    "        # SOLUTION: Step 2 - Classify product category using CLIP\n",
    "        product_categories = [\"clothing\", \"shoes\", \"electronics\", \"furniture\", \"books\", \"toys\"]\n",
    "        classification_results = classify_product_image(image, product_categories)\n",
    "        top_category = classification_results[0]  # Highest confidence category\n",
    "        \n",
    "        # SOLUTION: Step 3 - Generate product description using BLIP\n",
    "        description = generate_product_caption(image)\n",
    "        \n",
    "        # SOLUTION: Step 4 - Get category-specific questions and ask them\n",
    "        category = top_category['label']\n",
    "        questions = get_category_questions(category)\n",
    "        \n",
    "        # Ask each question and collect answers\n",
    "        qa_results = {}\n",
    "        for question in questions:\n",
    "            answer = ask_about_product(image, question)\n",
    "            qa_results[question] = answer\n",
    "        \n",
    "        # SOLUTION: Step 5 - Compile results into structured format\n",
    "        result = {\n",
    "            \"category\": {\n",
    "                \"name\": category,\n",
    "                \"confidence\": top_category['score']\n",
    "            },\n",
    "            \"description\": description,\n",
    "            \"attributes\": qa_results,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "        print(\"\\n‚úÖ Product analysis complete!\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during processing: {e}\")\n",
    "        return {\"error\": str(e), \"status\": \"failed\"}\n",
    "\n",
    "# TEST: Complete product analysis\n",
    "print(\"üöÄ TESTING: Complete product analysis pipeline...\")\n",
    "analysis_result = analyze_product(sample_url)\n",
    "print(\"Complete Analysis Result:\")\n",
    "print(analysis_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
