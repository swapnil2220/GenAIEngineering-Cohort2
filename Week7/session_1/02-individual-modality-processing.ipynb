{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 2: Individual Modality Processing\n",
    "=======================================\n",
    "\n",
    "[Click to view on Google Colab](https://colab.research.google.com/drive/1Q3IlQms7TbHDg2u0jOYMXqgGPVvFbXJw?usp=sharing)\n",
    "\n",
    "This script demonstrates how each modality (text, image, audio) gets preprocessed\n",
    "independently before being used in multimodal AI systems. We'll explore the\n",
    "transformation pipelines, normalization techniques, and modality-specific challenges.\n",
    "\n",
    "Learning Objectives:\n",
    "- Understand preprocessing steps for each modality\n",
    "- Learn about normalization and standardization techniques\n",
    "- Explore modality-specific challenges and solutions\n",
    "- See before/after transformations with dummy data\n",
    "- Understand why preprocessing is crucial for multimodal AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/ishandutta/miniconda3/envs/gs/lib/python3.10/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Processing\n",
    "\n",
    "\n",
    "#### Understanding Text Preprocessing in Multimodal AI\n",
    "\n",
    "\n",
    "Text preprocessing is the foundation of any text-based AI system. Unlike humans who can easily understand messy, inconsistent text, machine learning models require clean, standardized input data. The `TextProcessor` class demonstrates the essential steps that transform raw text into numerical representations that models can process.\n",
    "\n",
    "Key Concepts Covered:\n",
    "1. **Text Cleaning Challenges**  \n",
    "Raw text comes with inconsistencies: mixed case, extra spaces, punctuation, and special characters\n",
    "Different cleaning levels serve different purposes - basic cleaning preserves more information while advanced cleaning creates more uniform input\n",
    "Empty text and extremely long text require special handling to prevent model errors  \n",
    "2. **Tokenization Process**  \n",
    "Breaking text into meaningful units (tokens) is crucial for model understanding\n",
    "Special tokens like <START>, <END>, <PAD>, and <UNK> serve specific purposes in sequence processing\n",
    "Tokenization strategy affects how models interpret relationships between words  \n",
    "3. **Vocabulary Mapping**  \n",
    "Converting text tokens to numerical IDs enables mathematical operations\n",
    "Out-of-vocabulary words are handled gracefully using <UNK> tokens\n",
    "Vocabulary size directly impacts model complexity and memory requirements  \n",
    "4. **Sequence Standardization**  \n",
    "Variable-length text sequences must be standardized for batch processing\n",
    "Padding ensures all sequences have the same length for efficient computation\n",
    "Truncation prevents extremely long sequences from dominating processing time  \n",
    "  \n",
    "**Learning Outcomes:**  \n",
    "Learners will understand why text can't be fed directly to models, how preprocessing affects model performance, and the trade-offs between different cleaning strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTIMODAL AI - INDIVIDUAL MODALITY PROCESSING\n",
      "=======================================================\n",
      "Timestamp: 2025-08-16 15:55:37\n",
      "\n",
      "PART 1: TEXT PROCESSING\n",
      "==============================\n",
      "=== Text Processor Demonstration ===\n",
      "\n",
      "1. Text Processing Pipeline Demonstration:\n",
      "   Vocabulary size: 20\n",
      "   Max sequence length: 50\n",
      "\n",
      "Processing: clean_text\n",
      "Original: 'Hello world this is a clean text sample'\n",
      "Basic cleaned: 'hello world this is a clean text sample'\n",
      "Tokens: ['<START>', 'hello', 'world', 'this', 'is', 'a', 'clean', 'text', 'sample', '<END>']\n",
      "Token IDs: [2, 10, 11, 1, 1, 5, 1, 14, 1, 3]\n",
      "Final shape: (50,)\n",
      "--------------------------------------------------\n",
      "Processing: messy_text\n",
      "Original: '  HELLO!!! World???   This has    extra spaces & symbols!!!  '\n",
      "Basic cleaned: 'hello!!! world??? this has extra spaces & symbols!!!'\n",
      "Tokens: ['<START>', 'hello!!!', 'world???', 'this', 'has', 'extra', 'spaces', '&', 'symbols!!!', '<END>']\n",
      "Token IDs: [2, 1, 1, 1, 1, 1, 1, 1, 1, 3]\n",
      "Final shape: (50,)\n",
      "Advanced cleaned: 'hello world this has extra spaces symbols'\n",
      "--------------------------------------------------\n",
      "Processing: mixed_case\n",
      "Original: 'ThIs TeXt HaS mIxEd CaSe AnD needs NORMALIZATION'\n",
      "Basic cleaned: 'this text has mixed case and needs normalization'\n",
      "Tokens: ['<START>', 'this', 'text', 'has', 'mixed', 'case', 'and', 'needs', 'normalization', '<END>']\n",
      "Token IDs: [2, 1, 14, 1, 1, 1, 7, 1, 1, 3]\n",
      "Final shape: (50,)\n",
      "--------------------------------------------------\n",
      "Processing: with_numbers\n",
      "Original: 'The model achieved 95.5% accuracy on 1000 test samples in 2023'\n",
      "Basic cleaned: 'the model achieved 95.5% accuracy on 1000 test samples in 2023'\n",
      "Tokens: ['<START>', 'the', 'model', 'achieved', '95.5%', 'accuracy', 'on', '1000', 'test', 'samples']...\n",
      "Token IDs: [2, 4, 19, 1, 1, 1, 1, 1, 1, 1]...\n",
      "Final shape: (50,)\n",
      "--------------------------------------------------\n",
      "Processing: with_punctuation\n",
      "Original: 'Hello, world! How are you today? I'm fine, thanks.'\n",
      "Basic cleaned: 'hello, world! how are you today? i'm fine, thanks.'\n",
      "Tokens: ['<START>', 'hello,', 'world!', 'how', 'are', 'you', 'today?', \"i'm\", 'fine,', 'thanks.']...\n",
      "Token IDs: [2, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "Final shape: (50,)\n",
      "--------------------------------------------------\n",
      "Processing: empty_text\n",
      "Original: ''\n",
      "Basic cleaned: ''\n",
      "Tokens: []\n",
      "Token IDs: []\n",
      "Final shape: (50,)\n",
      "--------------------------------------------------\n",
      "Processing: very_long_text\n",
      "Original: 'word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word'\n",
      "Basic cleaned: 'word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word word'\n",
      "Tokens: ['<START>', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word']...\n",
      "Token IDs: [2, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "Final shape: (50,)\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    Handles text preprocessing operations including tokenization, normalization,\n",
    "    and feature extraction. This simulates real-world text processing pipelines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Text Processor\"\n",
    "        # Dummy vocabulary for demonstration (in real systems, this would be much larger)\n",
    "        self.vocabulary = {\n",
    "            '<PAD>': 0, \n",
    "            '<UNK>': 1, \n",
    "            '<START>': 2, \n",
    "            '<END>': 3,\n",
    "            'the': 4, \n",
    "            'a': 5, \n",
    "            'an': 6, \n",
    "            'and': 7, \n",
    "            'or': 8, \n",
    "            'but': 9,\n",
    "            'hello': 10, \n",
    "            'world': 11, \n",
    "            'ai': 12, \n",
    "            'multimodal': 13, \n",
    "            'text': 14,\n",
    "            'image': 15, \n",
    "            'audio': 16, \n",
    "            'data': 17, \n",
    "            'processing': 18, \n",
    "            'model': 19\n",
    "        }\n",
    "        self.max_sequence_length = 50\n",
    "        \n",
    "    def create_sample_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Creates sample texts with various preprocessing challenges\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of text samples with different characteristics\n",
    "        \"\"\"\n",
    "        samples = {\n",
    "            'clean_text': \"Hello world this is a clean text sample\",\n",
    "            'messy_text': \"  HELLO!!! World???   This has    extra spaces & symbols!!!  \",\n",
    "            'mixed_case': \"ThIs TeXt HaS mIxEd CaSe AnD needs NORMALIZATION\",\n",
    "            'with_numbers': \"The model achieved 95.5% accuracy on 1000 test samples in 2023\",\n",
    "            'with_punctuation': \"Hello, world! How are you today? I'm fine, thanks.\",\n",
    "            'empty_text': \"\",\n",
    "            'very_long_text': \" \".join([\"word\"] * 100)  # Simulate very long text\n",
    "        }\n",
    "        return samples\n",
    "    \n",
    "    def basic_text_cleaning(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Performs basic text cleaning operations\n",
    "        \n",
    "        Args:\n",
    "            text: Raw input text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text string\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to lowercase\n",
    "        cleaned = text.lower()\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        \n",
    "        # Remove leading/trailing whitespace\n",
    "        cleaned = cleaned.strip()\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def advanced_text_cleaning(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Performs advanced text cleaning including punctuation and special characters\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to clean\n",
    "            \n",
    "        Returns:\n",
    "            Advanced cleaned text\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "            \n",
    "        # Start with basic cleaning\n",
    "        cleaned = self.basic_text_cleaning(text)\n",
    "        \n",
    "        # Remove punctuation (keep alphanumeric and spaces)\n",
    "        cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned)\n",
    "        \n",
    "        # Handle numbers (replace with special token)\n",
    "        cleaned = re.sub(r'\\d+', '<NUM>', cleaned)\n",
    "        \n",
    "        # Remove extra spaces again after punctuation removal\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def tokenize_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes text into individual words/tokens\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            List of tokens\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "            \n",
    "        # Simple whitespace tokenization\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Add start and end tokens\n",
    "        tokens = ['<START>'] + tokens + ['<END>']\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Converts tokens to numerical IDs using vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of string tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of numerical token IDs\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            # Use vocabulary lookup, default to <UNK> if not found\n",
    "            token_id = self.vocabulary.get(token, self.vocabulary['<UNK>'])\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def pad_sequence(self, token_ids: List[int]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Pads or truncates sequence to fixed length\n",
    "        \n",
    "        Args:\n",
    "            token_ids: List of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            Padded/truncated sequence of fixed length\n",
    "        \"\"\"\n",
    "        if len(token_ids) > self.max_sequence_length:\n",
    "            # Truncate if too long\n",
    "            return token_ids[:self.max_sequence_length]\n",
    "        else:\n",
    "            # Pad if too short\n",
    "            padding_length = self.max_sequence_length - len(token_ids)\n",
    "            return token_ids + [self.vocabulary['<PAD>']] * padding_length\n",
    "    \n",
    "    def process_text_pipeline(self, text: str, cleaning_level: str = 'basic') -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete text processing pipeline\n",
    "        \n",
    "        Args:\n",
    "            text: Raw input text\n",
    "            cleaning_level: 'basic' or 'advanced' cleaning\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all processing steps and results\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'original_text': text,\n",
    "            'original_length': len(text),\n",
    "            'original_word_count': len(text.split()) if text else 0\n",
    "        }\n",
    "        \n",
    "        # Step 1: Text cleaning\n",
    "        if cleaning_level == 'advanced':\n",
    "            cleaned_text = self.advanced_text_cleaning(text)\n",
    "        else:\n",
    "            cleaned_text = self.basic_text_cleaning(text)\n",
    "        \n",
    "        result['cleaned_text'] = cleaned_text\n",
    "        result['cleaned_length'] = len(cleaned_text)\n",
    "        \n",
    "        # Step 2: Tokenization\n",
    "        tokens = self.tokenize_text(cleaned_text)\n",
    "        result['tokens'] = tokens\n",
    "        result['token_count'] = len(tokens)\n",
    "        \n",
    "        # Step 3: Token to ID conversion\n",
    "        token_ids = self.tokens_to_ids(tokens)\n",
    "        result['token_ids'] = token_ids\n",
    "        \n",
    "        # Step 4: Sequence padding/truncation\n",
    "        padded_sequence = self.pad_sequence(token_ids)\n",
    "        result['padded_sequence'] = padded_sequence\n",
    "        result['final_length'] = len(padded_sequence)\n",
    "        \n",
    "        # Step 5: Convert to numpy array (ready for model input)\n",
    "        result['model_input'] = np.array(padded_sequence, dtype=np.int32)\n",
    "        result['input_shape'] = result['model_input'].shape\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def demonstrate_text_processing(self):\n",
    "        \"\"\"\n",
    "        Main demonstration function for text processing\n",
    "        \"\"\"\n",
    "        print(f\"=== {self.name} Demonstration ===\\n\")\n",
    "        \n",
    "        # Get sample texts\n",
    "        sample_texts = self.create_sample_texts()\n",
    "        \n",
    "        print(\"1. Text Processing Pipeline Demonstration:\")\n",
    "        print(f\"   Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"   Max sequence length: {self.max_sequence_length}\\n\")\n",
    "        \n",
    "        for text_name, text in sample_texts.items():\n",
    "            print(f\"Processing: {text_name}\")\n",
    "            print(f\"Original: '{text}'\")\n",
    "            \n",
    "            # Process with basic cleaning\n",
    "            basic_result = self.process_text_pipeline(text, 'basic')\n",
    "            print(f\"Basic cleaned: '{basic_result['cleaned_text']}'\")\n",
    "            print(f\"Tokens: {basic_result['tokens'][:10]}{'...' if len(basic_result['tokens']) > 10 else ''}\")\n",
    "            print(f\"Token IDs: {basic_result['token_ids'][:10]}{'...' if len(basic_result['token_ids']) > 10 else ''}\")\n",
    "            print(f\"Final shape: {basic_result['input_shape']}\")\n",
    "            \n",
    "            # Show advanced cleaning for messy text\n",
    "            if text_name == 'messy_text':\n",
    "                advanced_result = self.process_text_pipeline(text, 'advanced')\n",
    "                print(f\"Advanced cleaned: '{advanced_result['cleaned_text']}'\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return sample_texts\n",
    "\n",
    "\n",
    "print(\"MULTIMODAL AI - INDIVIDUAL MODALITY PROCESSING\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "print(\"PART 1: TEXT PROCESSING\")\n",
    "print(\"=\" * 30)\n",
    "text_processor = TextProcessor()\n",
    "text_processor.demonstrate_text_processing()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data Processing\n",
    "\n",
    "**Image Preprocessing: From Pixels to Model Input**\n",
    "Images present unique challenges in multimodal AI due to their high dimensionality and variability in size, format, and quality. The ImageProcessor class demonstrates how raw pixel data is transformed into standardized numerical arrays suitable for deep learning models.  \n",
    "  \n",
    "**Key Concepts Covered:**  \n",
    "1. **Dimensional Standardization**  \n",
    "Images come in various sizes and aspect ratios, but models expect consistent input dimensions\n",
    "Resizing operations must balance information preservation with computational efficiency\n",
    "Channel handling (grayscale vs RGB vs RGBA) requires careful consideration\n",
    "2. **Pixel Value Normalization**  \n",
    "Raw pixel values (0-255) are scaled to ranges that optimize model training\n",
    "Standardization using dataset statistics (like ImageNet means/stds) improves model convergence\n",
    "Normalization prevents certain pixel ranges from dominating the learning process\n",
    "3. **Format Conversion Challenges**  \n",
    "Converting between color spaces (grayscale to RGB) affects information content\n",
    "Channel ordering (HWC vs CHW) must match model expectations\n",
    "Data type conversions (uint8 to float32) are necessary for mathematical operations\n",
    "4. **Data Augmentation Principles**  \n",
    "Augmentation techniques increase dataset diversity without collecting new data\n",
    "Transformations must preserve the essential visual information while adding variation\n",
    "Different augmentation strategies serve different purposes (geometric, photometric, etc.)\n",
    "5. **Memory and Computational Considerations**  \n",
    "Images consume significantly more memory than text\n",
    "Processing pipeline efficiency affects real-time application performance\n",
    "Batch dimension addition prepares data for efficient GPU processing\n",
    "  \n",
    "**Learning Outcomes:**  \n",
    "Students will appreciate the complexity of image preprocessing, understand why standardization is crucial, and recognize the computational trade-offs in image processing pipelines.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 2: IMAGE PROCESSING\n",
      "==============================\n",
      "=== Image Processor Demonstration ===\n",
      "\n",
      "1. Image Processing Pipeline Demonstration:\n",
      "   Target size: (224, 224)\n",
      "   Normalization mean: [0.485, 0.456, 0.406]\n",
      "   Normalization std: [0.229, 0.224, 0.225]\n",
      "\n",
      "Processing: small_grayscale\n",
      "Original shape: (64, 64)\n",
      "Original size: (64, 64)\n",
      "RGB conversion needed: True\n",
      "After RGB shape: (64, 64, 3)\n",
      "Resized shape: (224, 224, 3)\n",
      "Resize factors: (3.5, 3.5)\n",
      "Normalized range: (-2.118, 2.640)\n",
      "Final model input shape: (1, 3, 224, 224)\n",
      "Augmentation applied: False\n",
      "--------------------------------------------------\n",
      "Processing: large_rgb\n",
      "Original shape: (512, 768, 3)\n",
      "Original size: (512, 768)\n",
      "RGB conversion needed: False\n",
      "After RGB shape: (512, 768, 3)\n",
      "Resized shape: (224, 224, 3)\n",
      "Resize factors: (0.4375, 0.2916666666666667)\n",
      "Normalized range: (-2.118, 2.640)\n",
      "Final model input shape: (1, 3, 224, 224)\n",
      "Augmentation applied: False\n",
      "--------------------------------------------------\n",
      "Processing: square_rgb\n",
      "Original shape: (256, 256, 3)\n",
      "Original size: (256, 256)\n",
      "RGB conversion needed: False\n",
      "After RGB shape: (256, 256, 3)\n",
      "Resized shape: (224, 224, 3)\n",
      "Resize factors: (0.875, 0.875)\n",
      "Normalized range: (-2.118, 2.640)\n",
      "Final model input shape: (1, 3, 224, 224)\n",
      "Augmentation applied: True\n",
      "--------------------------------------------------\n",
      "Processing: very_small\n",
      "Original shape: (32, 48, 3)\n",
      "Original size: (32, 48)\n",
      "RGB conversion needed: False\n",
      "After RGB shape: (32, 48, 3)\n",
      "Resized shape: (224, 224, 3)\n",
      "Resize factors: (7.0, 4.666666666666667)\n",
      "Normalized range: (-2.118, 2.640)\n",
      "Final model input shape: (1, 3, 224, 224)\n",
      "Augmentation applied: False\n",
      "--------------------------------------------------\n",
      "Processing: high_values\n",
      "Original shape: (128, 128, 3)\n",
      "Original size: (128, 128)\n",
      "RGB conversion needed: False\n",
      "After RGB shape: (128, 128, 3)\n",
      "Resized shape: (224, 224, 3)\n",
      "Resize factors: (1.75, 1.75)\n",
      "Normalized range: (-2.118, 2.640)\n",
      "Final model input shape: (1, 3, 224, 224)\n",
      "Augmentation applied: False\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ImageProcessor:\n",
    "    \"\"\"\n",
    "    Handles image preprocessing operations including resizing, normalization,\n",
    "    and augmentation. This simulates real-world image processing pipelines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Image Processor\"\n",
    "        self.target_size = (224, 224)  # Common input size for many models\n",
    "        self.normalization_mean = [0.485, 0.456, 0.406]  # ImageNet means\n",
    "        self.normalization_std = [0.229, 0.224, 0.225]   # ImageNet stds\n",
    "        \n",
    "    def create_sample_images(self) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Creates sample images with various preprocessing challenges\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of image samples with different characteristics\n",
    "        \"\"\"\n",
    "        samples = {\n",
    "            'small_grayscale': {\n",
    "                'data': np.random.randint(0, 256, (64, 64), dtype=np.uint8),\n",
    "                'channels': 1,\n",
    "                'original_size': (64, 64)\n",
    "            },\n",
    "            'large_rgb': {\n",
    "                'data': np.random.randint(0, 256, (512, 768, 3), dtype=np.uint8),\n",
    "                'channels': 3,\n",
    "                'original_size': (512, 768)\n",
    "            },\n",
    "            'square_rgb': {\n",
    "                'data': np.random.randint(0, 256, (256, 256, 3), dtype=np.uint8),\n",
    "                'channels': 3,\n",
    "                'original_size': (256, 256)\n",
    "            },\n",
    "            'very_small': {\n",
    "                'data': np.random.randint(0, 256, (32, 48, 3), dtype=np.uint8),\n",
    "                'channels': 3,\n",
    "                'original_size': (32, 48)\n",
    "            },\n",
    "            'high_values': {\n",
    "                'data': np.random.randint(200, 256, (128, 128, 3), dtype=np.uint8),\n",
    "                'channels': 3,\n",
    "                'original_size': (128, 128)\n",
    "            }\n",
    "        }\n",
    "        return samples\n",
    "    \n",
    "    def resize_image(self, image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulates image resizing (simplified version)\n",
    "        In real applications, this would use proper interpolation\n",
    "        \n",
    "        Args:\n",
    "            image: Input image array\n",
    "            target_size: Target (height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Resized image array\n",
    "        \"\"\"\n",
    "        original_shape = image.shape\n",
    "        \n",
    "        if len(original_shape) == 2:  # Grayscale\n",
    "            # Simple nearest neighbor simulation\n",
    "            resized = np.random.randint(0, 256, target_size, dtype=np.uint8)\n",
    "        else:  # RGB/RGBA\n",
    "            channels = original_shape[2]\n",
    "            resized = np.random.randint(0, 256, (*target_size, channels), dtype=np.uint8)\n",
    "        \n",
    "        return resized\n",
    "    \n",
    "    def normalize_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalizes image pixel values to [0, 1] range and applies standardization\n",
    "        \n",
    "        Args:\n",
    "            image: Input image array (0-255 range)\n",
    "            \n",
    "        Returns:\n",
    "            Normalized image array\n",
    "        \"\"\"\n",
    "        # Convert to float and normalize to [0, 1]\n",
    "        normalized = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Apply channel-wise standardization if RGB\n",
    "        if len(normalized.shape) == 3 and normalized.shape[2] == 3:\n",
    "            for i in range(3):\n",
    "                normalized[:, :, i] = (normalized[:, :, i] - self.normalization_mean[i]) / self.normalization_std[i]\n",
    "        elif len(normalized.shape) == 2:  # Grayscale\n",
    "            # Simple standardization for grayscale\n",
    "            normalized = (normalized - 0.5) / 0.5\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def handle_grayscale_to_rgb(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts grayscale image to RGB by replicating channels\n",
    "        \n",
    "        Args:\n",
    "            image: Grayscale image array\n",
    "            \n",
    "        Returns:\n",
    "            RGB image array\n",
    "        \"\"\"\n",
    "        if len(image.shape) == 2:\n",
    "            # Replicate grayscale channel to create RGB\n",
    "            rgb_image = np.stack([image, image, image], axis=2)\n",
    "            return rgb_image\n",
    "        return image\n",
    "    \n",
    "    def apply_basic_augmentation(self, image: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Applies basic data augmentation techniques (simulated)\n",
    "        \n",
    "        Args:\n",
    "            image: Input image array\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of augmented images\n",
    "        \"\"\"\n",
    "        augmented = {}\n",
    "        \n",
    "        # Original\n",
    "        augmented['original'] = image.copy()\n",
    "        \n",
    "        # Horizontal flip (simulated)\n",
    "        augmented['horizontal_flip'] = np.fliplr(image)\n",
    "        \n",
    "        # Brightness adjustment (simulated)\n",
    "        bright_factor = 1.2\n",
    "        augmented['brightness_up'] = np.clip(image * bright_factor, 0, 255).astype(image.dtype)\n",
    "        \n",
    "        # Contrast adjustment (simulated)\n",
    "        contrast_factor = 1.3\n",
    "        mean_val = np.mean(image)\n",
    "        augmented['contrast_up'] = np.clip((image - mean_val) * contrast_factor + mean_val, 0, 255).astype(image.dtype)\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def process_image_pipeline(self, image_data: Dict[str, Any], apply_augmentation: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete image processing pipeline\n",
    "        \n",
    "        Args:\n",
    "            image_data: Dictionary containing image data and metadata\n",
    "            apply_augmentation: Whether to apply data augmentation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all processing steps and results\n",
    "        \"\"\"\n",
    "        image = image_data['data']\n",
    "        \n",
    "        result = {\n",
    "            'original_shape': image.shape,\n",
    "            'original_dtype': image.dtype,\n",
    "            'original_size_bytes': image.nbytes,\n",
    "            'original_value_range': (int(image.min()), int(image.max()))\n",
    "        }\n",
    "        \n",
    "        # Step 1: Handle grayscale to RGB conversion if needed\n",
    "        if len(image.shape) == 2:\n",
    "            image = self.handle_grayscale_to_rgb(image)\n",
    "            result['converted_to_rgb'] = True\n",
    "        else:\n",
    "            result['converted_to_rgb'] = False\n",
    "        \n",
    "        result['after_rgb_conversion_shape'] = image.shape\n",
    "        \n",
    "        # Step 2: Resize image\n",
    "        resized_image = self.resize_image(image, self.target_size)\n",
    "        result['resized_shape'] = resized_image.shape\n",
    "        result['resize_factor'] = (\n",
    "            self.target_size[0] / image.shape[0],\n",
    "            self.target_size[1] / image.shape[1]\n",
    "        )\n",
    "        \n",
    "        # Step 3: Normalize image\n",
    "        normalized_image = self.normalize_image(resized_image)\n",
    "        result['normalized_dtype'] = normalized_image.dtype\n",
    "        result['normalized_value_range'] = (float(normalized_image.min()), float(normalized_image.max()))\n",
    "        \n",
    "        # Step 4: Convert to model input format (add batch dimension)\n",
    "        # Transpose from HWC to CHW format (common in deep learning)\n",
    "        if len(normalized_image.shape) == 3:\n",
    "            model_input = np.transpose(normalized_image, (2, 0, 1))  # CHW format\n",
    "        else:\n",
    "            model_input = normalized_image\n",
    "        \n",
    "        # Add batch dimension\n",
    "        model_input = np.expand_dims(model_input, axis=0)\n",
    "\n",
    "        # [H, W, C] -> [B, H, W, C]\n",
    "        # [128, 128, 3] -> [1, 128, 128, 3] d\n",
    "        \n",
    "        result['model_input_shape'] = model_input.shape\n",
    "        result['model_input'] = model_input\n",
    "        \n",
    "        # Step 5: Apply augmentation if requested\n",
    "        if apply_augmentation:\n",
    "            augmented_images = self.apply_basic_augmentation(resized_image)\n",
    "            result['augmented_versions'] = len(augmented_images)\n",
    "            result['augmentation_applied'] = True\n",
    "        else:\n",
    "            result['augmentation_applied'] = False\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def demonstrate_image_processing(self):\n",
    "        \"\"\"\n",
    "        Main demonstration function for image processing\n",
    "        \"\"\"\n",
    "        print(f\"=== {self.name} Demonstration ===\\n\")\n",
    "        \n",
    "        # Get sample images\n",
    "        sample_images = self.create_sample_images()\n",
    "        \n",
    "        print(\"1. Image Processing Pipeline Demonstration:\")\n",
    "        print(f\"   Target size: {self.target_size}\")\n",
    "        print(f\"   Normalization mean: {self.normalization_mean}\")\n",
    "        print(f\"   Normalization std: {self.normalization_std}\\n\")\n",
    "        \n",
    "        for image_name, image_data in sample_images.items():\n",
    "            print(f\"Processing: {image_name}\")\n",
    "            print(f\"Original shape: {image_data['data'].shape}\")\n",
    "            print(f\"Original size: {image_data['original_size']}\")\n",
    "            \n",
    "            # Process image\n",
    "            result = self.process_image_pipeline(image_data, apply_augmentation=(image_name == 'square_rgb'))\n",
    "            \n",
    "            print(f\"RGB conversion needed: {result['converted_to_rgb']}\")\n",
    "            print(f\"After RGB shape: {result['after_rgb_conversion_shape']}\")\n",
    "            print(f\"Resized shape: {result['resized_shape']}\")\n",
    "            print(f\"Resize factors: {result['resize_factor']}\")\n",
    "            print(f\"Normalized range: ({result['normalized_value_range'][0]:.3f}, {result['normalized_value_range'][1]:.3f})\")\n",
    "            print(f\"Final model input shape: {result['model_input_shape']}\")\n",
    "            print(f\"Augmentation applied: {result['augmentation_applied']}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return sample_images\n",
    "\n",
    "\n",
    "print(\"PART 2: IMAGE PROCESSING\")\n",
    "print(\"=\" * 30)\n",
    "image_processor = ImageProcessor()\n",
    "image_processor.demonstrate_image_processing()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Data Processing\n",
    "\n",
    "**Audio Signal Processing for AI Applications**\n",
    "Audio data presents temporal challenges that differ from both text and images. The AudioProcessor class demonstrates how continuous audio signals are converted into discrete, standardized representations suitable for machine learning models.\n",
    "  \n",
    "**Key Concepts Covered:**  \n",
    "1. **Temporal Data Characteristics**  \n",
    "Audio is inherently time-series data with temporal dependencies\n",
    "Sample rates determine the resolution of audio capture and affect processing requirements\n",
    "Duration variability requires standardization strategies similar to text sequence lengths\n",
    "2. **Channel and Format Standardization**  \n",
    "Stereo to mono conversion affects information content but simplifies processing\n",
    "Sample rate conversion (resampling) must preserve essential frequency information\n",
    "Amplitude normalization prevents volume variations from affecting model training\n",
    "3. **Spectral Feature Extraction**  \n",
    "Time-domain audio signals are often converted to frequency-domain representations\n",
    "STFT (Short-Time Fourier Transform) captures both temporal and spectral information\n",
    "Mel spectrograms and MFCCs provide perceptually-relevant audio features\n",
    "Feature extraction transforms 1D audio into 2D representations suitable for various model architectures\n",
    "4. **Windowing and Framing**  \n",
    "Audio signals are processed in overlapping windows to capture temporal dynamics\n",
    "Window size and hop length parameters balance temporal resolution with computational efficiency\n",
    "Windowing functions reduce spectral artifacts in frequency analysis\n",
    "5. **Length Standardization**  \n",
    "Audio clips of varying lengths must be standardized for batch processing\n",
    "Padding with silence vs. truncation affects information preservation\n",
    "Fixed-length processing enables efficient model training and inference\n",
    "6. **Signal Quality Considerations**  \n",
    "Amplitude normalization prevents clipping and ensures consistent signal levels\n",
    "Noise handling and filtering improve signal quality for model input\n",
    "Dynamic range considerations affect model sensitivity to quiet vs. loud sounds\n",
    "  \n",
    "**Learning Outcomes:**  \n",
    "Students will understand the unique challenges of temporal data processing, appreciate the complexity of audio feature extraction, and recognize why spectral representations are often preferred over raw audio for AI applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 3: AUDIO PROCESSING\n",
      "==============================\n",
      "=== Audio Processor Demonstration ===\n",
      "\n",
      "1. Audio Processing Pipeline Demonstration:\n",
      "   Target sample rate: 16000 Hz\n",
      "   Window size: 1024\n",
      "   Hop length: 512\n",
      "   Mel frequency bins: 128\n",
      "\n",
      "Processing: short_mono_16k\n",
      "Original: 1.0s, 16000Hz, 1 ch\n",
      "Converted to mono: False\n",
      "Resampling ratio: 1.00\n",
      "Amplitude range after norm: (-1.000, 1.000)\n",
      "Padding applied: False\n",
      "Truncation applied: False\n",
      "Features extracted: True\n",
      "STFT shape: (513, 31)\n",
      "Mel spectrogram shape: (128, 31)\n",
      "MFCC shape: (13, 31)\n",
      "Final model input shape: (128, 31)\n",
      "--------------------------------------------------\n",
      "Processing: long_stereo_44k\n",
      "Original: 3.0s, 44100Hz, 2 ch\n",
      "Converted to mono: True\n",
      "Resampling ratio: 0.36\n",
      "Amplitude range after norm: (-1.000, 1.000)\n",
      "Padding applied: False\n",
      "Truncation applied: True\n",
      "Features extracted: True\n",
      "STFT shape: (513, 31)\n",
      "Mel spectrogram shape: (128, 31)\n",
      "MFCC shape: (13, 31)\n",
      "Final model input shape: (128, 31)\n",
      "--------------------------------------------------\n",
      "Processing: quiet_audio\n",
      "Original: 1.0s, 22050Hz, 1 ch\n",
      "Converted to mono: False\n",
      "Resampling ratio: 0.73\n",
      "Amplitude range after norm: (-1.000, 1.000)\n",
      "Padding applied: False\n",
      "Truncation applied: False\n",
      "Features extracted: False\n",
      "Final model input shape: (16000,)\n",
      "--------------------------------------------------\n",
      "Processing: loud_audio\n",
      "Original: 1.0s, 8000Hz, 1 ch\n",
      "Converted to mono: False\n",
      "Resampling ratio: 2.00\n",
      "Amplitude range after norm: (-1.000, 1.000)\n",
      "Padding applied: False\n",
      "Truncation applied: False\n",
      "Features extracted: False\n",
      "Final model input shape: (16000,)\n",
      "--------------------------------------------------\n",
      "Processing: very_short\n",
      "Original: 0.1s, 16000Hz, 1 ch\n",
      "Converted to mono: False\n",
      "Resampling ratio: 1.00\n",
      "Amplitude range after norm: (-1.000, 0.999)\n",
      "Padding applied: True\n",
      "Truncation applied: False\n",
      "Features extracted: False\n",
      "Final model input shape: (16000,)\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"\n",
    "    Handles audio preprocessing operations including resampling, windowing,\n",
    "    and feature extraction. This simulates real-world audio processing pipelines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Audio Processor\"\n",
    "        self.target_sample_rate = 16000  # Common sample rate for speech processing\n",
    "        self.window_size = 1024  # For spectral analysis\n",
    "        self.hop_length = 512    # For spectral analysis\n",
    "        self.n_mels = 128        # Number of mel frequency bins\n",
    "        \n",
    "    def create_sample_audio(self) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Creates sample audio with various preprocessing challenges\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of audio samples with different characteristics\n",
    "        \"\"\"\n",
    "        samples = {\n",
    "            'short_mono_16k': {\n",
    "                'data': np.random.uniform(-1.0, 1.0, 16000),  # 1 second\n",
    "                'sample_rate': 16000,\n",
    "                'channels': 1,\n",
    "                'duration': 1.0\n",
    "            },\n",
    "            'long_stereo_44k': {\n",
    "                'data': np.random.uniform(-1.0, 1.0, (44100 * 3, 2)),  # 3 seconds stereo\n",
    "                'sample_rate': 44100,\n",
    "                'channels': 2,\n",
    "                'duration': 3.0\n",
    "            },\n",
    "            'quiet_audio': {\n",
    "                'data': np.random.uniform(-0.1, 0.1, 22050),  # Very quiet audio\n",
    "                'sample_rate': 22050,\n",
    "                'channels': 1,\n",
    "                'duration': 1.0\n",
    "            },\n",
    "            'loud_audio': {\n",
    "                'data': np.random.uniform(-0.9, 0.9, 8000),  # Loud audio, low sample rate\n",
    "                'sample_rate': 8000,\n",
    "                'channels': 1,\n",
    "                'duration': 1.0\n",
    "            },\n",
    "            'very_short': {\n",
    "                'data': np.random.uniform(-0.5, 0.5, 1600),  # 0.1 seconds\n",
    "                'sample_rate': 16000,\n",
    "                'channels': 1,\n",
    "                'duration': 0.1\n",
    "            }\n",
    "        }\n",
    "        return samples\n",
    "    \n",
    "    def resample_audio(self, audio: np.ndarray, original_sr: int, target_sr: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulates audio resampling (simplified version)\n",
    "        In real applications, this would use proper signal processing\n",
    "        \n",
    "        Args:\n",
    "            audio: Input audio array\n",
    "            original_sr: Original sample rate\n",
    "            target_sr: Target sample rate\n",
    "            \n",
    "        Returns:\n",
    "            Resampled audio array\n",
    "        \"\"\"\n",
    "        if original_sr == target_sr:\n",
    "            return audio\n",
    "        \n",
    "        # Calculate resampling ratio\n",
    "        ratio = target_sr / original_sr\n",
    "        \n",
    "        if len(audio.shape) == 1:  # Mono\n",
    "            new_length = int(len(audio) * ratio)\n",
    "            resampled = np.random.uniform(-1.0, 1.0, new_length)\n",
    "        else:  # Stereo\n",
    "            new_length = int(audio.shape[0] * ratio)\n",
    "            resampled = np.random.uniform(-1.0, 1.0, (new_length, audio.shape[1]))\n",
    "        \n",
    "        return resampled\n",
    "    \n",
    "    def convert_to_mono(self, audio: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts stereo audio to mono by averaging channels\n",
    "        \n",
    "        Args:\n",
    "            audio: Input audio array\n",
    "            \n",
    "        Returns:\n",
    "            Mono audio array\n",
    "        \"\"\"\n",
    "        if len(audio.shape) == 1:\n",
    "            return audio  # Already mono\n",
    "        else:\n",
    "            # Average across channels\n",
    "            return np.mean(audio, axis=1)\n",
    "    \n",
    "    def normalize_audio(self, audio: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalizes audio amplitude\n",
    "        \n",
    "        Args:\n",
    "            audio: Input audio array\n",
    "            \n",
    "        Returns:\n",
    "            Normalized audio array\n",
    "        \"\"\"\n",
    "        # Find the maximum absolute value\n",
    "        max_val = np.max(np.abs(audio))\n",
    "        \n",
    "        if max_val > 0:\n",
    "            # Normalize to [-1, 1] range\n",
    "            normalized = audio / max_val\n",
    "        else:\n",
    "            normalized = audio\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def apply_windowing(self, audio: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Applies windowing function to audio signal\n",
    "        \n",
    "        Args:\n",
    "            audio: Input audio array\n",
    "            \n",
    "        Returns:\n",
    "            Windowed audio array\n",
    "        \"\"\"\n",
    "        # Apply Hanning window (simulated)\n",
    "        window = np.hanning(len(audio))\n",
    "        windowed = audio * window\n",
    "        return windowed\n",
    "    \n",
    "    def extract_spectral_features(self, audio: np.ndarray, sample_rate: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Extracts spectral features from audio (simulated)\n",
    "        \n",
    "        Args:\n",
    "            audio: Input audio array\n",
    "            sample_rate: Sample rate of audio\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of spectral features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Simulate STFT (Short-Time Fourier Transform)\n",
    "        n_frames = max(1, len(audio) // self.hop_length)\n",
    "        n_freq_bins = self.window_size // 2 + 1\n",
    "        \n",
    "        features['stft_magnitude'] = np.random.uniform(0, 1, (n_freq_bins, n_frames))\n",
    "        features['stft_phase'] = np.random.uniform(-np.pi, np.pi, (n_freq_bins, n_frames))\n",
    "        \n",
    "        # Simulate Mel spectrogram\n",
    "        features['mel_spectrogram'] = np.random.uniform(0, 1, (self.n_mels, n_frames))\n",
    "        \n",
    "        # Simulate MFCCs (Mel-Frequency Cepstral Coefficients)\n",
    "        n_mfcc = 13\n",
    "        features['mfcc'] = np.random.uniform(-1, 1, (n_mfcc, n_frames))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def pad_or_truncate_audio(self, audio: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pads or truncates audio to target length\n",
    "        \n",
    "        Args:\n",
    "            audio: Input audio array\n",
    "            target_length: Target length in samples\n",
    "            \n",
    "        Returns:\n",
    "            Padded or truncated audio array\n",
    "        \"\"\"\n",
    "        current_length = len(audio)\n",
    "        \n",
    "        if current_length > target_length:\n",
    "            # Truncate\n",
    "            return audio[:target_length]\n",
    "        elif current_length < target_length:\n",
    "            # Pad with zeros\n",
    "            padding = target_length - current_length\n",
    "            return np.pad(audio, (0, padding), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            return audio\n",
    "    \n",
    "    def process_audio_pipeline(self, audio_data: Dict[str, Any], extract_features: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete audio processing pipeline\n",
    "        \n",
    "        Args:\n",
    "            audio_data: Dictionary containing audio data and metadata\n",
    "            extract_features: Whether to extract spectral features\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all processing steps and results\n",
    "        \"\"\"\n",
    "        audio = audio_data['data']\n",
    "        original_sr = audio_data['sample_rate']\n",
    "        \n",
    "        result = {\n",
    "            'original_shape': audio.shape,\n",
    "            'original_sample_rate': original_sr,\n",
    "            'original_duration': audio_data['duration'],\n",
    "            'original_channels': audio_data['channels'],\n",
    "            'original_amplitude_range': (float(audio.min()), float(audio.max()))\n",
    "        }\n",
    "        \n",
    "        # Step 1: Convert to mono if stereo\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = self.convert_to_mono(audio)\n",
    "            result['converted_to_mono'] = True\n",
    "        else:\n",
    "            result['converted_to_mono'] = False\n",
    "        \n",
    "        result['after_mono_shape'] = audio.shape\n",
    "        \n",
    "        # Step 2: Resample to target sample rate\n",
    "        resampled_audio = self.resample_audio(audio, original_sr, self.target_sample_rate)\n",
    "        result['resampled_shape'] = resampled_audio.shape\n",
    "        result['resampling_ratio'] = self.target_sample_rate / original_sr\n",
    "        \n",
    "        # Step 3: Normalize audio\n",
    "        normalized_audio = self.normalize_audio(resampled_audio)\n",
    "        result['normalized_amplitude_range'] = (float(normalized_audio.min()), float(normalized_audio.max()))\n",
    "        \n",
    "        # Step 4: Pad or truncate to fixed length (1 second at target sample rate)\n",
    "        target_length = self.target_sample_rate  # 1 second\n",
    "        fixed_length_audio = self.pad_or_truncate_audio(normalized_audio, target_length)\n",
    "        result['fixed_length_shape'] = fixed_length_audio.shape\n",
    "        result['padding_applied'] = len(fixed_length_audio) > len(normalized_audio)\n",
    "        result['truncation_applied'] = len(normalized_audio) > target_length\n",
    "        \n",
    "        # Step 5: Apply windowing\n",
    "        windowed_audio = self.apply_windowing(fixed_length_audio)\n",
    "        result['windowing_applied'] = True\n",
    "        \n",
    "        # Step 6: Extract spectral features if requested\n",
    "        if extract_features:\n",
    "            spectral_features = self.extract_spectral_features(windowed_audio, self.target_sample_rate)\n",
    "            result['spectral_features'] = {\n",
    "                'stft_shape': spectral_features['stft_magnitude'].shape,\n",
    "                'mel_spectrogram_shape': spectral_features['mel_spectrogram'].shape,\n",
    "                'mfcc_shape': spectral_features['mfcc'].shape\n",
    "            }\n",
    "            result['features_extracted'] = True\n",
    "            \n",
    "            # Model input would typically be one of these features\n",
    "            result['model_input'] = spectral_features['mel_spectrogram']\n",
    "            result['model_input_shape'] = result['model_input'].shape\n",
    "        else:\n",
    "            result['features_extracted'] = False\n",
    "            result['model_input'] = windowed_audio\n",
    "            result['model_input_shape'] = windowed_audio.shape\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def demonstrate_audio_processing(self):\n",
    "        \"\"\"\n",
    "        Main demonstration function for audio processing\n",
    "        \"\"\"\n",
    "        print(f\"=== {self.name} Demonstration ===\\n\")\n",
    "        \n",
    "        # Get sample audio\n",
    "        sample_audio = self.create_sample_audio()\n",
    "        \n",
    "        print(\"1. Audio Processing Pipeline Demonstration:\")\n",
    "        print(f\"   Target sample rate: {self.target_sample_rate} Hz\")\n",
    "        print(f\"   Window size: {self.window_size}\")\n",
    "        print(f\"   Hop length: {self.hop_length}\")\n",
    "        print(f\"   Mel frequency bins: {self.n_mels}\\n\")\n",
    "        \n",
    "        for audio_name, audio_data in sample_audio.items():\n",
    "            print(f\"Processing: {audio_name}\")\n",
    "            print(f\"Original: {audio_data['duration']}s, {audio_data['sample_rate']}Hz, {audio_data['channels']} ch\")\n",
    "            \n",
    "            # Process audio\n",
    "            result = self.process_audio_pipeline(audio_data, extract_features=(audio_name in ['short_mono_16k', 'long_stereo_44k']))\n",
    "            \n",
    "            print(f\"Converted to mono: {result['converted_to_mono']}\")\n",
    "            print(f\"Resampling ratio: {result['resampling_ratio']:.2f}\")\n",
    "            print(f\"Amplitude range after norm: ({result['normalized_amplitude_range'][0]:.3f}, {result['normalized_amplitude_range'][1]:.3f})\")\n",
    "            print(f\"Padding applied: {result['padding_applied']}\")\n",
    "            print(f\"Truncation applied: {result['truncation_applied']}\")\n",
    "            print(f\"Features extracted: {result['features_extracted']}\")\n",
    "            \n",
    "            if result['features_extracted']:\n",
    "                print(f\"STFT shape: {result['spectral_features']['stft_shape']}\")\n",
    "                print(f\"Mel spectrogram shape: {result['spectral_features']['mel_spectrogram_shape']}\")\n",
    "                print(f\"MFCC shape: {result['spectral_features']['mfcc_shape']}\")\n",
    "            \n",
    "            print(f\"Final model input shape: {result['model_input_shape']}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return sample_audio\n",
    "\n",
    "\n",
    "print(\"PART 3: AUDIO PROCESSING\")\n",
    "print(\"=\" * 30)\n",
    "audio_processor = AudioProcessor()\n",
    "audio_processor.demonstrate_audio_processing()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modality Processing Comparison\n",
    "\n",
    "**Cross-Modal Preprocessing Analysis**\n",
    "The comparison section synthesizes the preprocessing challenges and solutions across all three modalities, highlighting both commonalities and unique requirements. This analysis is crucial for understanding why multimodal AI systems are complex and how different data types interact.\n",
    "  \n",
    "**Key Concepts Covered:**  \n",
    "1. **Universal Preprocessing Principles**  \n",
    "Standardization: All modalities require conversion to consistent formats\n",
    "Normalization: Scaling values to appropriate ranges is universal across modalities\n",
    "Dimensionality Management: Fixed input dimensions enable efficient batch processing\n",
    "Quality Control: Noise reduction and irrelevant information filtering improve model performance\n",
    "2. **Modality-Specific Challenges**  \n",
    "Text: Discrete symbols with semantic relationships, variable sequence lengths\n",
    "Images: Continuous pixel values with spatial relationships, high dimensionality\n",
    "Audio: Continuous temporal signals with frequency content, time-series dependencies\n",
    "3. **Resource and Computational Considerations**  \n",
    "Memory usage varies dramatically across modalities (text < audio < images)\n",
    "Processing complexity differs based on data characteristics and required transformations\n",
    "Real-time processing constraints affect preprocessing pipeline design\n",
    "4. **Information Preservation Trade-offs**  \n",
    "Each preprocessing step potentially loses information\n",
    "Balancing standardization needs with information retention is crucial\n",
    "Different applications may require different preprocessing strategies\n",
    "5. **Preprocessing Impact on Model Performance**  \n",
    "Quality of preprocessing directly affects downstream model performance\n",
    "Inconsistent preprocessing can introduce biases and reduce model robustness\n",
    "Understanding preprocessing effects is essential for debugging model issues\n",
    "6. **Scalability and Production Considerations**  \n",
    "Preprocessing pipelines must handle varying data quality in production\n",
    "Error handling and fallback mechanisms are essential for robust systems\n",
    "Preprocessing efficiency affects overall system performance\n",
    "  \n",
    "**Learning Outcomes:**  \n",
    "Students will understand the fundamental principles that apply across all modalities, appreciate the unique challenges each data type presents, and recognize why multimodal systems require careful coordination of preprocessing pipelines. This foundation prepares them for understanding how different modalities can be combined effectively in multimodal AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 4: PROCESSING COMPARISON\n",
      "==============================\n",
      "=== Modality Processing Comparison ===\n",
      "\n",
      "1. Common Preprocessing Challenges by Modality:\n",
      "\n",
      "TEXT PROCESSING CHALLENGES:\n",
      "    Variable sequence lengths  Solution: Padding/Truncation\n",
      "    Different vocabularies  Solution: Vocabulary mapping\n",
      "    Case sensitivity  Solution: Normalization\n",
      "    Punctuation and special characters  Solution: Cleaning\n",
      "    Out-of-vocabulary words  Solution: <UNK> tokens\n",
      "    Multiple languages  Solution: Multilingual tokenizers\n",
      "\n",
      "IMAGE PROCESSING CHALLENGES:\n",
      "    Variable image sizes  Solution: Resizing\n",
      "    Different color spaces  Solution: Standardization\n",
      "    Pixel value ranges  Solution: Normalization\n",
      "    Different aspect ratios  Solution: Cropping/Padding\n",
      "    Limited training data  Solution: Data augmentation\n",
      "    Channel differences  Solution: Channel conversion\n",
      "\n",
      "AUDIO PROCESSING CHALLENGES:\n",
      "    Variable sample rates  Solution: Resampling\n",
      "    Different audio lengths  Solution: Padding/Truncation\n",
      "    Stereo vs mono  Solution: Channel conversion\n",
      "    Amplitude variations  Solution: Normalization\n",
      "    Background noise  Solution: Filtering\n",
      "    Temporal dependencies  Solution: Windowing/Framing\n",
      "\n",
      "2. Key Preprocessing Principles:\n",
      "    STANDARDIZATION: Convert all inputs to consistent format\n",
      "    NORMALIZATION: Scale values to appropriate ranges\n",
      "    DIMENSIONALITY: Ensure consistent input dimensions\n",
      "    QUALITY: Remove noise and irrelevant information\n",
      "    AUGMENTATION: Increase data diversity when needed\n",
      "\n",
      "3. Modality-Specific Considerations:\n",
      "    Text: Semantic meaning preserved during cleaning\n",
      "    Image: Spatial relationships maintained during resizing\n",
      "    Audio: Temporal information preserved during processing\n",
      "\n",
      "============================================================\n",
      "DEMONSTRATION COMPLETE\n",
      "\n",
      "Key Takeaways:\n",
      "- Each modality requires specific preprocessing steps\n",
      "- Standardization and normalization are crucial for all modalities\n",
      "- Variable input sizes must be handled consistently\n",
      "- Preprocessing quality directly affects model performance\n",
      "- Understanding modality-specific challenges is essential\n"
     ]
    }
   ],
   "source": [
    "class ModalityProcessingComparison:\n",
    "    \"\"\"\n",
    "    Compares preprocessing challenges and solutions across modalities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Modality Processing Comparison\"\n",
    "    \n",
    "    def compare_preprocessing_challenges(self):\n",
    "        \"\"\"\n",
    "        Compares preprocessing challenges across different modalities\n",
    "        \"\"\"\n",
    "        print(f\"=== {self.name} ===\\n\")\n",
    "        \n",
    "        print(\"1. Common Preprocessing Challenges by Modality:\\n\")\n",
    "        \n",
    "        print(\"TEXT PROCESSING CHALLENGES:\")\n",
    "        print(\"    Variable sequence lengths  Solution: Padding/Truncation\")\n",
    "        print(\"    Different vocabularies  Solution: Vocabulary mapping\")\n",
    "        print(\"    Case sensitivity  Solution: Normalization\")\n",
    "        print(\"    Punctuation and special characters  Solution: Cleaning\")\n",
    "        print(\"    Out-of-vocabulary words  Solution: <UNK> tokens\")\n",
    "        print(\"    Multiple languages  Solution: Multilingual tokenizers\")\n",
    "        \n",
    "        print(\"\\nIMAGE PROCESSING CHALLENGES:\")\n",
    "        print(\"    Variable image sizes  Solution: Resizing\")\n",
    "        print(\"    Different color spaces  Solution: Standardization\")\n",
    "        print(\"    Pixel value ranges  Solution: Normalization\")\n",
    "        print(\"    Different aspect ratios  Solution: Cropping/Padding\")\n",
    "        print(\"    Limited training data  Solution: Data augmentation\")\n",
    "        print(\"    Channel differences  Solution: Channel conversion\")\n",
    "        \n",
    "        print(\"\\nAUDIO PROCESSING CHALLENGES:\")\n",
    "        print(\"    Variable sample rates  Solution: Resampling\")\n",
    "        print(\"    Different audio lengths  Solution: Padding/Truncation\")\n",
    "        print(\"    Stereo vs mono  Solution: Channel conversion\")\n",
    "        print(\"    Amplitude variations  Solution: Normalization\")\n",
    "        print(\"    Background noise  Solution: Filtering\")\n",
    "        print(\"    Temporal dependencies  Solution: Windowing/Framing\")\n",
    "        \n",
    "        print(\"\\n2. Key Preprocessing Principles:\")\n",
    "        print(\"    STANDARDIZATION: Convert all inputs to consistent format\")\n",
    "        print(\"    NORMALIZATION: Scale values to appropriate ranges\")\n",
    "        print(\"    DIMENSIONALITY: Ensure consistent input dimensions\")\n",
    "        print(\"    QUALITY: Remove noise and irrelevant information\")\n",
    "        print(\"    AUGMENTATION: Increase data diversity when needed\")\n",
    "        \n",
    "        print(\"\\n3. Modality-Specific Considerations:\")\n",
    "        print(\"    Text: Semantic meaning preserved during cleaning\")\n",
    "        print(\"    Image: Spatial relationships maintained during resizing\")\n",
    "        print(\"    Audio: Temporal information preserved during processing\")\n",
    "\n",
    "\n",
    "print(\"PART 4: PROCESSING COMPARISON\")\n",
    "print(\"=\" * 30)\n",
    "comparison = ModalityProcessingComparison()\n",
    "comparison.compare_preprocessing_challenges()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEMONSTRATION COMPLETE\")\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"- Each modality requires specific preprocessing steps\")\n",
    "print(\"- Standardization and normalization are crucial for all modalities\")\n",
    "print(\"- Variable input sizes must be handled consistently\")\n",
    "print(\"- Preprocessing quality directly affects model performance\")\n",
    "print(\"- Understanding modality-specific challenges is essential\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "**Why Individual Modality Processing Matters?**  \n",
    "Understanding individual modality processing is essential before tackling multimodal fusion because:  \n",
    "- **Foundation Building:** Each modality has unique characteristics that must be properly handled\n",
    "- **Quality Assurance:** Poor preprocessing in any modality degrades overall system performance\n",
    "- **Debugging Capability:** Understanding individual pipelines enables effective troubleshooting\n",
    "- **Design Decisions:** Preprocessing choices affect how modalities can be combined later\n",
    "- **Resource Planning:** Different modalities have different computational and memory requirements  \n",
    "  \n",
    "This comprehensive understanding of individual modality processing provides the necessary foundation for exploring how these different data types can be effectively combined in multimodal AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
